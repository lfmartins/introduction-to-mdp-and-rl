{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPFOHDpmX8GfjKTEr7lvK0E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction"],"metadata":{"id":"TTcgBzkSK_hR"}},{"cell_type":"markdown","source":["## This is here just to test a commit!!!!"],"metadata":{"id":"u7_k1jgIq6ev"}},{"cell_type":"markdown","source":["In this notebook, we introduce *Tiny Robot*, which is a simple example of Markov Decision Process (MDP). Although the example is simple, it actually represents a general discrete time MDP with a finite number of states.\n","\n","We will first present code to represent the MDP in \"pure Python\". This way, we can explore the key concepts in MDPs without the complexity of a full-fledged library. Later, we will see how to code and solve the Tiny Robot problem using TF-Agents, the powerful TensorFlow library for RL.\n","\n","Run the cell below to import the modules and functions used in the notebook:"],"metadata":{"id":"Ywc--7iHLCzv"}},{"cell_type":"code","source":["# Numpy: efficient multidimensional arrays\n","import numpy as np\n","\n","# matplotlib: interactive plots\n","import matplotlib.pyplot as plt"],"metadata":{"id":"PmZDvyH2Mkz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tiny Robot"],"metadata":{"id":"MmK17sUe4DC7"}},{"cell_type":"markdown","source":["Tiny Robot lives in an environment consisting of four rooms arranged in a $4\\times4$ grid. We number the rooms $0$, $1$, $2$ and $3$, clockwise starting with the upper-left room.\n","\n","In each time step, the robot can stay in the same room or move to an adjacent room. The robot can influence to which room it is going to move in the next step by choosing an action. There are two possible actions, numbered $0$ and $1$.\n","\n","We will concentrate in an example in which there are two actions, numbered $0$ and $1$:\n","\n","- If action $0$ is selected, the robot stays in the same room with probability $2/3$, or moves to the next room in the clockwise direction with probability $1/3$.\n","\n","- If action $1$ is selected, the robot moves to one of the adjacent rooms, each with probability $1/2$\n","\n","These specifications can be represented by a transition probability matrix, given by the following tables:\n","\n","<center>\n","\n","| Action 0 |          |          |          |          ||| Action 1 |          |          |          |          |\n",":---------:|:--------:|:--------:|:--------:|:--------:|||:--------:|:--------:|:--------:|:--------:|:--------:|\n","|          |0         |1         |2         |3         |||          |0         |1         |2         |3         |\n","|0         |2/3       |1/3       |0         |0         |||0         |0         |1/2       |0         |1/2       |\n","|1         |0         |2/3       |1/3       |0         |||1         |1/2       |0         |1/2       |0         |\n","|2         |0         |0         |2/3       |1/3       |||2         |0         |1/2       |0         |1/2       |\n","|3         |1/3       |0         |0         |2/3       |||3         |1/2       |0         |1/2       |0         |\n","\n","</center>\n","\n","In the tables above, rows represent the current room the robot is in, and the columns represent the room the robot moves to in the next step.\n","\n","When visiting a room, the robot performs a task and receives a reward for it. The rewards are specified in the next table. The reward depends on both the room and the chosen action, according to the following table:\n","\n","<center>\n","\n","| Rewards  |          |          |          |           |\n","|:---------|:--------:|:--------:|:--------:|:---------:|\n","| Room     |0         |1         |2         |3          |\n","| Action 0 |10        |20        |30        |40         |\n","| Action 1 |40        |30        |20        |30         |\n","\n","</center>\n","\n","The objective of the robot is to maximize the average cumulative reward in a sequence of steps. Of course, if we let the robot run indefinitely, it can accumulate an infinite reward. Later, we will see how this can be limited. For now, let's assume that the robot runs for a finite number of steps. Each run of the robot is called an *episode*.\n","\n","We assume that the robot starts in a random room, with all rooms equally likely."],"metadata":{"id":"f9g_gdwF4G54"}},{"cell_type":"markdown","source":["# Representation of the Environment"],"metadata":{"id":"nzEjWv5uHL8n"}},{"cell_type":"markdown","source":["Let's now consider the problem of representing the environment of an MDP in Python. At this point, we don't use any libraries, to make the presentation simpler. Later, we will see how to do it in TF-Agents, the TensorFlow library for RL.\n","\n","We define a class `MDPEnvironment` that can represent an arbitrary discrete time MDP with finite state space. To do this, we need to represent the following:\n","\n","- *The transition probability matrix for each action*. These will be stored in a \"rank 3 tensor\", that is, a 3-dimensional Numpy array `tprobs`. The element `tprobs[a,i,j]` represents the probability of a transition from state `i` to state `j` when action `a` is selected.\n","\n","- *The rewards for every action-state pair*. These are represented by a \"rank 2 tensor\" `rewards`. Element `reward[a,i]`\n","represents the reward received when selecting action `a` in state `i`.\n","\n","We also need to specify how the process is started. We assume that the starting room is randomized from some probability distribution. This distribution is given by the parameter `init_distr` in the class initializer.\n","\n","Here is the class definition:"],"metadata":{"id":"n55iu38zHQl0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4dTwYjX3-k6"},"outputs":[],"source":["class MDPEnvironment(object):\n","\n","    def __init__(self, tprobs, rewards, init_distr, seed=0):\n","        self._tprobs = np.array(tprobs, dtype=np.float64)\n","        self._rewards = np.array(rewards, dtype=np.float64)\n","        self._init_distr = np.array(init_distr, dtype=np.float64)\n","\n","        # Check array dimensions\n","        if self._tprobs.ndim != 3:\n","            raise ValueError('tprobs must be a 3-dimensional array')\n","        if self._rewards.ndim != 2:\n","            raise ValueError('rewards must be a 2-dimensional array')\n","        if self._init_distr.ndim != 1:\n","            raise ValueError('init_distr must be a 1-dimensional array')\n","        if self._tprobs.shape[0] != self._rewards.shape[0]:\n","            raise ValueError('axis 0 of arrays tprobs and rewards must have the same length')\n","        if self._tprobs.shape[1] != self._tprobs.shape[2]:\n","            raise ValueError('axes 1 and 2 of array tprobs must have the same length')\n","        if self._tprobs.shape[1] != self._rewards.shape[1]:\n","            raise ValueError('axis 1 of arrays tprobs and rewards must have the same length')\n","        if self._tprobs.shape[1] != self._init_distr.shape[0]:\n","            raise ValueError('axis 1 of array tprobs must have the same length as init_distr')\n","\n","        self._num_actions = self._tprobs.shape[0]\n","        self._num_states = self._tprobs.shape[1]\n","\n","        # Define random number generator used to simulate process\n","        self._rng = np.random.default_rng(seed=seed)\n","\n","        # Start in an unitialized state\n","        self._current_state = None\n","        self._current_reward = None\n","        self._current_status = 0\n","\n","    def set_seed(self, seed):\n","        self._rng = np.random.default_rng(seed=seed)\n","\n","    def transition_probs(self):\n","        return self._tprobs.copy()\n","\n","    def rewards(self):\n","        return self._rewards.copy()\n","\n","    def status(self):\n","        return self._current_status\n","\n","    def current_state(self):\n","        return self._current_state\n","\n","    def current_reward(self):\n","        return self._current_reward\n","\n","    def reset(self):\n","        self._current_status = 1\n","        self._current_state = self._rng.choice(self._num_states, p=init_distr)\n","        self._current_reward = 0.0\n","\n","    def step(self, action):\n","        if action < 0 or action >= self._tprobs.shape[0]:\n","            raise ValueError(f'action must be an integer between 0 and {self._tprobs.shape[0]}')\n","        self._current_reward = self._rewards[action, self._current_state]\n","        self._current_state = self._rng.choice(self._num_states, p=self._tprobs[action, self._current_state])"]},{"cell_type":"markdown","source":["Here are the details of the implementation:\n","\n","- When defining an object of class `MDPEnvironment`, the following arguments should be passed:\n","\n","    - `tprobs` a three-dimensional array with shape $(N_a,N_s,N_s)$, where $N_a$ is the number of actions and $N_s$ is the number of states. For every action, the rows of the array `tprobs[action, :, :]` must be a valid transition probability matrix (its elements are non-negative and rows add to one).\n","\n","    - `rewards` is a two dimensional array of shape $(N_a, N_s)$\n","\n","    - `init_distr` is a one dimensional array of shape $(N_s,)$ whose elements are non-negative and add to $1$.\n","\n","- Information about characteristics of the environment can be accessed with the methods `transition_probs()` and `rewards()`. These return the corresponding arrays used to initialize the MDP.\n","\n","- Method `status()` returns the status of a run. Status $0$ means that the chain has not been initialized. Status 1 means that an episode is being run.\n","\n","- Method `current_state()` returns in which state the environment is currently in.\n","\n","- Method `current_reward()` returns the reward that resulted after the latest action was executer. This is initialized to $0$.\n","\n","- Method `reset()` reinitializes the MDP, and should be called before running an episode.\n","\n","- Method `step()` takes an action as input and simulates one step of the MDP. It updates the state of the chain and the current reward.\n","\n","- Method `set_seed()` sets the seed of the random number generator. It can be used to simulate identical runs.\n","\n","As an example, let's create an environment for Tiny Robot:"],"metadata":{"id":"IN9ksMe4Sici"}},{"cell_type":"code","source":["# Define the transition probabilities\n","tprobs = [\n","    [\n","        [2/3, 1/3, 0.0, 0.0],\n","        [0.0, 2/3, 1/3, 0.0],\n","        [0.0, 0.0, 2/3, 1/3],\n","        [1/3, 0.0, 0.0, 2/3]\n","    ],\n","    [\n","        [0.0, 1/2, 0.0, 1/2],\n","        [1/2, 0.0, 1/2, 0.0],\n","        [0.0, 1/2, 0.0, 1/2],\n","        [1/2, 0.0, 1/2, 0.0]\n","    ]\n","]\n","\n","# Define the rewards\n","rewards = [\n","    [40, 30, 20, 10],\n","    [10, 20, 30, 40]\n","]\n","\n","# Define initial state distribution\n","init_distr = [1/4, 1/4, 1/4, 1/4]\n","\n","# Create the Tiny Robot Environment\n","tr_env = MDPEnvironment(tprobs, rewards, init_distr, seed=77)"],"metadata":{"id":"9AbuWuI6PPaW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that, we create the environment, we can specify a seed. This seed is used by the internal random number generator used by the class `MDPEnvironment`. This is useful if we want to repeat experiments with exactly the same outcomes. The seed can also be set with the method `set_seed()`\n","\n","We can get information about the environment with the following functions:"],"metadata":{"id":"6cdXs1gSYbpW"}},{"cell_type":"code","source":["print(\"Transition Probabilities\")\n","tr_env.transition_probs()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RvtrCcrJPXg2","executionInfo":{"status":"ok","timestamp":1689139456897,"user_tz":240,"elapsed":10,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"e1dcf1e1-8c99-4cdb-c9a0-160c1be064d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Transition Probabilities\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[[0.66666667, 0.33333333, 0.        , 0.        ],\n","        [0.        , 0.66666667, 0.33333333, 0.        ],\n","        [0.        , 0.        , 0.66666667, 0.33333333],\n","        [0.33333333, 0.        , 0.        , 0.66666667]],\n","\n","       [[0.        , 0.5       , 0.        , 0.5       ],\n","        [0.5       , 0.        , 0.5       , 0.        ],\n","        [0.        , 0.5       , 0.        , 0.5       ],\n","        [0.5       , 0.        , 0.5       , 0.        ]]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["print(\"Rewards\")\n","tr_env.rewards()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6i1XmuRIPhk9","executionInfo":{"status":"ok","timestamp":1689139456897,"user_tz":240,"elapsed":9,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"9328bb37-2c52-4379-f1ca-ff7fbd21524f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rewards\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[40., 30., 20., 10.],\n","       [10., 20., 30., 40.]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Current status, state and reward\n","print(tr_env.status(), tr_env.current_state(), tr_env.current_reward())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7DxnpXwZWsZ","executionInfo":{"status":"ok","timestamp":1689139456897,"user_tz":240,"elapsed":7,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"b224ac0e-2f6c-4dfb-db5c-547e9ffcc384"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 None None\n"]}]},{"cell_type":"markdown","source":["Status 0 indicates that a run has not been started, and the chain in an undefined state. This is indicated by both the current state and current reward being `None`.\n","\n","To start a simularuntion, we first call the method `reset()`. This randomizes the initial state according to the initial distributions, and the sets current reward to zero."],"metadata":{"id":"8lNVz6XXZjxc"}},{"cell_type":"code","source":["tr_env.reset()\n","print(f'Current state: {tr_env.current_state()}; Current reward: {tr_env.current_reward()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5OwcfcwZhnp","executionInfo":{"status":"ok","timestamp":1689139456897,"user_tz":240,"elapsed":6,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"377fbc80-d1bf-4826-8647-ac9df83f9aaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current state: 3; Current reward: 0.0\n"]}]},{"cell_type":"markdown","source":["We can now call the `step()` method to simulate the process. Let's say we choose action $1$:"],"metadata":{"id":"DFuCdMJXdj4K"}},{"cell_type":"code","source":["tr_env.step(1)\n","print(f'State: {tr_env.current_state()}, reward: {tr_env.current_reward()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I2q-UFN8dyEQ","executionInfo":{"status":"ok","timestamp":1689139456898,"user_tz":240,"elapsed":6,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"93eef265-a46e-48cb-9d81-cfc371544e51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 2, reward: 40.0\n"]}]},{"cell_type":"markdown","source":["Notice the value of the reward. The MDP starts in state $3$, and action $1$ is chosen. Thus, the reward in the first step is `rewards[1, 3] == 40`. It is important to notice that the current reward is a function of the previous state and the last action chosen.\n","\n","Also notice the status of the simulation:"],"metadata":{"id":"yhdLheXPd_Zy"}},{"cell_type":"code","source":["tr_env.status()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQNBC7AoEd7M","executionInfo":{"status":"ok","timestamp":1689139456898,"user_tz":240,"elapsed":5,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"7ce513ac-54fe-4a20-ec08-37fd0f01f7d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The status has changed to $1$, indicating that the MDP is in the middle of a run. In more complex examples, this can be used to check if the run has started. The exercises explore the addition of termination conditions.\n","\n","Let's simulate a few more steps, alternating between actions $0$ and $1$:"],"metadata":{"id":"ULEo_P58EVBs"}},{"cell_type":"code","source":["tr_env.step(0)\n","print(f'State: {tr_env.current_state()}, reward: {tr_env.current_reward()}')\n","tr_env.step(1)\n","print(f'State: {tr_env.current_state()}, reward: {tr_env.current_reward()}')\n","tr_env.step(0)\n","print(f'State: {tr_env.current_state()}, reward: {tr_env.current_reward()}')\n","tr_env.step(1)\n","print(f'State: {tr_env.current_state()}, reward: {tr_env.current_reward()}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tvvcr8WseHIF","executionInfo":{"status":"ok","timestamp":1689139456898,"user_tz":240,"elapsed":4,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"01ba1db8-c9d5-4938-d161-9a12a15083b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 2, reward: 20.0\n","State: 1, reward: 30.0\n","State: 1, reward: 30.0\n","State: 0, reward: 20.0\n"]}]},{"cell_type":"markdown","source":["Let's suppose that each run consists of $10$ steps. Let's simulate a run, assuming that actions $0$ and $1$ are chosen with equal probability:"],"metadata":{"id":"ww6jR4bP6Wbb"}},{"cell_type":"code","source":["# Reset the seeds\n","tr_env.set_seed(77)\n","rng = np.random.default_rng(seed=77)\n","\n","num_steps = 10\n","cumulative_reward = 0.0\n","\n","# Reset the environment\n","tr_env.reset()\n","\n","# Simulate one run\n","for t in range(num_steps):\n","    action = rng.integers(2)\n","    tr_env.step(action)\n","    cumulative_reward += tr_env.current_reward()\n","    print(f'Step: {t + 1:2d}, state: {tr_env.current_state()}, '\n","          f'reward: {tr_env.current_reward()}, cumulative reward: {cumulative_reward}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15-ZaAsTfPh0","executionInfo":{"status":"ok","timestamp":1689139457070,"user_tz":240,"elapsed":175,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"d1050534-ed87-42ae-9a8b-e4b147a2a18d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step:  1, state: 3, reward: 10.0, cumulative reward: 10.0\n","Step:  2, state: 0, reward: 40.0, cumulative reward: 50.0\n","Step:  3, state: 1, reward: 10.0, cumulative reward: 60.0\n","Step:  4, state: 0, reward: 20.0, cumulative reward: 80.0\n","Step:  5, state: 1, reward: 10.0, cumulative reward: 90.0\n","Step:  6, state: 2, reward: 30.0, cumulative reward: 120.0\n","Step:  7, state: 1, reward: 30.0, cumulative reward: 150.0\n","Step:  8, state: 1, reward: 30.0, cumulative reward: 180.0\n","Step:  9, state: 2, reward: 20.0, cumulative reward: 200.0\n","Step: 10, state: 3, reward: 20.0, cumulative reward: 220.0\n"]}]},{"cell_type":"markdown","source":["In an MDP, we want to maximize the average cumulative reward over many episodes. Let's simulate the Tiny Robot for several episodes and record the average reward per episode."],"metadata":{"id":"duC6FtGyHEkm"}},{"cell_type":"code","source":["# Reset the seeds\n","tr_env.set_seed(77)\n","rng = np.random.default_rng(seed=77)\n","\n","num_steps = 10\n","num_episodes = 200\n","avg_rewards = [0.0]\n","\n","for n in range(num_episodes):\n","    # Reset the environment\n","    tr_env.reset()\n","    cumulative_reward = 0.0\n","\n","    # Simulate one episode\n","    for t in range(num_steps):\n","        action = rng.integers(2)\n","        tr_env.step(action)\n","        cumulative_reward += tr_env.current_reward()\n","\n","    # Update average reward per episode\n","    avg_rewards.append(avg_rewards[-1] + 1 / (n + 1) * (cumulative_reward - avg_rewards[-1]))\n","\n","print(f'Average cumulative reward per episode: {avg_rewards[-1]:8.5f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jov7nOTzHm4Y","executionInfo":{"status":"ok","timestamp":1689139457070,"user_tz":240,"elapsed":2,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"6e27badc-291d-433a-851d-515714d47750"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average cumulative reward per episode: 246.05000\n"]}]},{"cell_type":"markdown","source":["Let's plot the evolution of the average rewards:"],"metadata":{"id":"uWMltGUiJaOe"}},{"cell_type":"code","source":["plt.figure(figsize=(6,4))\n","plt.plot(range(len(avg_rewards)), avg_rewards)\n","plt.title('Average cumulative reward per episode')\n","plt.xlabel('Episode')\n","plt.ylabel('Average reward')\n","None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"xjZCfl-kJgHq","executionInfo":{"status":"ok","timestamp":1689139457439,"user_tz":240,"elapsed":370,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"119d4ede-f8a1-469a-fe1f-9f7bab152eee"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAhwAAAGJCAYAAADBveoRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABd40lEQVR4nO3deXgT1foH8G+SNkn3facUaBHKqhQplbLzoyyKQJVVAS+yFkUR5eLCJgpyFblyEcQr4EUQ8YILyHKRHSmL7GtlKRToSkubrmmTnN8fNVNCCzTYIS39fp4nD+TMZOadmTR5c86ZcxRCCAEiIiIiGSltHQARERE9+phwEBERkeyYcBAREZHsmHAQERGR7JhwEBERkeyYcBAREZHsmHAQERGR7JhwEBERkeyYcBAREZHsmHAQ1TIzZsyAQqGo0m3u2rULCoUCu3btqtLt1ka1/Vx26tQJnTp1eqj7XLFiBRQKBa5cufJQ91vbMOGge/r888+hUCgQGRlp61CoGvj888+xYsUKW4dBRDWQna0DoOpt1apVqFevHg4dOoSLFy8iLCzM1iGRDX3++efw9vbGiBEjLMo7dOiAwsJCqNVq2wRGj4z//e9/tg6BZMIaDrqrxMRE7N+/H/Pnz4ePjw9WrVr10GMwmUwoKip66Psl6yiVSmi1WiiVtv9Iyc/Pt3UI9ySEQGFhoa3DqLSCgoKHuj+1Ws3E9RFl+08HqrZWrVoFDw8P9O7dG88995xFwlFSUgJPT0+89NJL5V6n0+mg1WoxefJkqUyv12P69OkICwuDRqNBcHAw3nrrLej1eovXKhQKTJgwAatWrULTpk2h0WiwZcsWAMDHH3+Mp556Cl5eXnBwcEBERAT++9//ltt/YWEhXn31VXh7e8PFxQV9+vTBjRs3oFAoMGPGDIt1b9y4gb/97W/w8/ODRqNB06ZNsWzZskqfo2+++QZt2rSBo6MjPDw80KFDB4tfaBXtEwDq1atnUUtgbkPet28fXn31Vfj4+MDd3R1jxoxBcXExsrOzMWzYMHh4eMDDwwNvvfUWbp/o+W7t/leuXIFCobhvM8jy5cvRpUsX+Pr6QqPRoEmTJli8eHG5mM+cOYPdu3dDoVBAoVBIbe137n/ChAlwdnau8Mtq8ODB8Pf3h9FolMo2b96M9u3bw8nJCS4uLujduzfOnDlzz5hvP2+7d+/G+PHj4evrizp16lR6uz///DMUCgVOnjwpla1btw4KhQL9+/e32Fd4eDgGDhxo1Tkzn7enn34aW7duRevWreHg4IAvvvgCAHD9+nX07dsXTk5O8PX1xeuvv17ub+JuzH1xzp8/jwEDBsDV1RVeXl6YOHFihUn6N998g4iICDg4OMDT0xODBg3CtWvXLNbp1KkTmjVrhiNHjqBDhw5wdHTE22+/fc84zp8/j+eeew6enp7QarVo3bo1fv75Z4t1zNdpz549GDNmDLy8vODq6ophw4bh1q1b5WK4sw/HwoUL0bRpU+nvrHXr1li9erXFOseOHUPPnj3h6uoKZ2dndO3aFQcOHCgX75kzZ9ClSxc4ODigTp06mD17NkwmU4XH9qDvS6oYm1TorlatWoX+/ftDrVZj8ODBWLx4MQ4fPownn3wS9vb26NevH9avX48vvvjC4hfJjz/+CL1ej0GDBgEoraXo06cP9u3bh9GjRyM8PBynTp3Cp59+ij/++AM//vijxX537NiBtWvXYsKECfD29ka9evUAAP/85z/Rp08fDB06FMXFxVizZg2ef/55bNy4Eb1795ZeP2LECKxduxYvvvgi2rZti927d1ssN0tLS0Pbtm2lJMfHxwebN2/GyJEjodPp8Nprr93z/MycORMzZszAU089hVmzZkGtVuPgwYPYsWMHunfv/kDn/JVXXoG/vz9mzpyJAwcOYOnSpXB3d8f+/ftRt25dfPjhh9i0aRP+8Y9/oFmzZhg2bNgD7edOixcvRtOmTdGnTx/Y2dlhw4YNGD9+PEwmE+Li4gAACxYswCuvvAJnZ2e88847AAA/P78Ktzdw4EAsWrQIv/zyC55//nmpvKCgABs2bMCIESOgUqkAACtXrsTw4cMRExODjz76CAUFBVi8eDGio6Nx7Ngx6frfy/jx4+Hj44Np06ZJNRyV2W50dLT0RdiiRQsAwN69e6FUKrFv3z5p+xkZGTh//jwmTJhg1TkzS0hIwODBgzFmzBiMGjUKjRo1QmFhIbp27YqkpCS8+uqrCAwMxMqVK7Fjx477Hu/tBgwYgHr16mHOnDk4cOAAPvvsM9y6dQv/+c9/pHU++OADvPfeexgwYABefvllZGRkYOHChejQoQOOHTsGd3d3ad3MzEz07NkTgwYNwgsvvHDXawyUfnm3a9cOQUFB+Pvf/w4nJyesXbsWffv2xbp169CvXz+L9SdMmAB3d3fMmDEDCQkJWLx4Ma5evSolrBX58ssv8eqrr+K5556TkqmTJ0/i4MGDGDJkiBRH+/bt4erqirfeegv29vb44osv0KlTJ+zevVvqg5aamorOnTvDYDBI8S5duhQODg7l9lsV70u6gyCqwO+//y4AiG3btgkhhDCZTKJOnTpi4sSJ0jpbt24VAMSGDRssXturVy/RoEED6fnKlSuFUqkUe/futVhvyZIlAoD47bffpDIAQqlUijNnzpSLqaCgwOJ5cXGxaNasmejSpYtUduTIEQFAvPbaaxbrjhgxQgAQ06dPl8pGjhwpAgICxM2bNy3WHTRokHBzcyu3v9tduHBBKJVK0a9fP2E0Gi2WmUwmi+O5fZ9mISEhYvjw4dLz5cuXCwAiJibG4vVRUVFCoVCIsWPHSmUGg0HUqVNHdOzYUSrbuXOnACB27txpsZ/ExEQBQCxfvlwqmz59urjzT7+iY42JibG4jkII0bRpU4v93m3/JpNJBAUFidjYWIv11q5dKwCIPXv2CCGEyM3NFe7u7mLUqFEW66Wmpgo3N7dy5Xcyn7fo6GhhMBikcmu227RpUzFgwADpeatWrcTzzz8vAIhz584JIYRYv369ACBOnDghrVfZcxYSEiIAiC1btliUL1iwQAAQa9eulcry8/NFWFhYhdfyTubr2KdPH4vy8ePHW8R65coVoVKpxAcffGCx3qlTp4SdnZ1FeceOHQUAsWTJknvu26xr166iefPmoqioSCozmUziqaeeEg0bNpTKzNcpIiJCFBcXS+Xz5s0TAMRPP/1kEcPt77Fnn31WNG3a9J5x9O3bV6jVanHp0iWpLDk5Wbi4uIgOHTpIZa+99poAIA4ePCiVpaenCzc3NwFAJCYmCiH++vuSKsYmFarQqlWr4Ofnh86dOwMobRoYOHAg1qxZI1WFd+nSBd7e3vjuu++k1926dQvbtm2zqHr+/vvvER4ejsaNG+PmzZvSo0uXLgCAnTt3Wuy7Y8eOaNKkSbmYbv8VcuvWLeTk5KB9+/Y4evSoVG5ufhk/frzFa1955RWL50IIrFu3Ds888wyEEBZxxcTEICcnx2K7d/rxxx9hMpkwbdq0cv0W/sotpyNHjrR4fWRkJIQQGDlypFSmUqnQunVrXL58+YH3c6fbz21OTg5u3ryJjh074vLly8jJybF6ewqFAs8//zw2bdqEvLw8qfy7775DUFAQoqOjAQDbtm1DdnY2Bg8ebHENVCoVIiMjy7037mbUqFFSjYm1223fvj327t0LAMjNzcWJEycwevRoeHt7S+V79+6Fu7s7mjVr9kDnrH79+oiJibEo27RpEwICAvDcc89JZY6Ojhg9enSljtnsztoU83t906ZNAID169fDZDJhwIABFufC398fDRs2LHeONRpNhU2ld8rKysKOHTswYMAA5ObmStvNzMxETEwMLly4gBs3bli8ZvTo0bC3t5eejxs3DnZ2dlKsFXF3d8f169dx+PDhCpcbjUb873//Q9++fdGgQQOpPCAgAEOGDMG+ffug0+mkc9K2bVu0adNGWs/HxwdDhw612GZVvS/JEptUqByj0Yg1a9agc+fOSExMlMojIyPxySefYPv27ejevTvs7OwQGxuL1atXQ6/XQ6PRYP369SgpKbFIOC5cuIBz587Bx8enwv2lp6dbPK9fv36F623cuBGzZ8/G8ePHLdq5b/+Cvnr1KpRKZblt3Hl3TUZGBrKzs7F06VIsXbq0UnHd7tKlS1AqlRUmRn9F3bp1LZ67ubkBAIKDg8uV39n2/Vf89ttvmD59OuLj48v1u8jJyZHisMbAgQOxYMEC/PzzzxgyZAjy8vKwadMmjBkzRrpmFy5cAAAp+byTq6trpfZ15/W2Zrvt27fHkiVLcPHiRVy6dAkKhQJRUVFSIjJq1Cjs3bsX7dq1s0gurTlnFb2nr169irCwsHIJaqNGjSp1zGYNGza0eB4aGgqlUimNKXHhwgUIIcqtZ3Z7AgAAQUFBleq0efHiRQgh8N577+G9996rcJ309HQEBQXdNVZnZ2cEBATcc/yLKVOm4Ndff0WbNm0QFhaG7t27Y8iQIWjXrh2A0r/lgoKCCs9beHg4TCYTrl27hqZNm+Lq1asV3uJ/52ur6n1JlphwUDk7duxASkoK1qxZgzVr1pRbvmrVKqmPwqBBg/DFF19g8+bN6Nu3L9auXYvGjRujZcuW0vomkwnNmzfH/PnzK9zfnV+mFbWn7t27F3369EGHDh3w+eefIyAgAPb29li+fHm5zmOVYe4k9sILL2D48OEVrmNu05fD7R0mb3f7r/T7lYvbOo3erVblbvu53aVLl9C1a1c0btwY8+fPR3BwMNRqNTZt2oRPP/30rh3q7qdt27aoV68e1q5diyFDhmDDhg0oLCy0SEbN2165ciX8/f3LbcPOrnIfUXe+Z6zZrrm2Zc+ePbh8+TJatWoFJycntG/fHp999hny8vJw7NgxfPDBB9JrrD1nFb2n5XLne8FkMkGhUGDz5s0Vvo+cnZ0tnlc2VvMxTp48uVztjVlV3EYfHh6OhIQEbNy4EVu2bMG6devw+eefY9q0aZg5c+Zf3n5Fqup9SZZ41qicVatWwdfXF4sWLSq3bP369fjhhx+wZMkSODg4oEOHDggICMB3332H6Oho7NixQ+pQaBYaGooTJ06ga9euD9zcsG7dOmi1WmzduhUajUYqX758ucV6ISEhMJlMSExMtPg1dfHiRYv1fHx84OLiAqPRiG7dulkdT2hoKEwmE86ePYvHH3/8rut5eHggOzvboqy4uBgpKSlW7/NePDw8AKDcvq5evXrf127YsAF6vR4///yzRQ1LRdXG1l6/AQMG4J///Cd0Oh2+++471KtXD23btpWWh4aGAgB8fX0f6DrcjTXbrVu3LurWrYu9e/fi8uXLaN++PYDSsUUmTZqE77//HkajER06dJBeY805u5uQkBCcPn0aQgiL85qQkFDpbQClv8Zvr0G5ePEiTCaT1KkxNDQUQgjUr18fjz32mFXbvhdz84W9vX2lr92FCxekZloAyMvLQ0pKCnr16nXP1zk5OWHgwIEYOHAgiouL0b9/f3zwwQeYOnUqfHx84OjoWOF5O3/+PJRKpfSjJiQkRKq9uN2dr5XrfVnbsQ8HWSgsLMT69evx9NNP47nnniv3mDBhAnJzc6Xb3pRKJZ577jls2LABK1euhMFgsPgFC5R+6dy4cQNffvllhfurzLgJKpUKCoXC4hf7lStXyt3hYv6l9fnnn1uUL1y4sNz2YmNjsW7dOpw+fbrc/jIyMu4ZT9++faFUKjFr1qxyv2Zvr3kIDQ3Fnj17LJYvXbq0UjUP1ggJCYFKpSq3rzvPQ0XMv3pvjzsnJ6dcMgeUfvDfmdTcy8CBA6HX6/H1119jy5YtGDBggMXymJgYuLq64sMPP0RJSUm519/vOtyNtdtt3749duzYgUOHDkkJx+OPPw4XFxfMnTtXug3bzJpzdje9evVCcnKyxa3dBQUFd23iu5s7fxiY3+s9e/YEAPTv3x8qlQozZ860iNccf2ZmplX7M/P19UWnTp3wxRdfVJhAV3Ttli5danE9Fi9eDIPBIMVakTvjU6vVaNKkCYQQKCkpgUqlQvfu3fHTTz9ZNM2kpaVh9erViI6OlppAevXqhQMHDuDQoUMWcd45xpBc78vajjUcZOHnn39Gbm4u+vTpU+Hytm3bSoOAmROLgQMHYuHChZg+fTqaN2+O8PBwi9e8+OKLWLt2LcaOHYudO3eiXbt2MBqNOH/+PNauXSuNT3AvvXv3xvz589GjRw8MGTIE6enpWLRoEcLCwizGUIiIiEBsbCwWLFiAzMxM6bbYP/74A4DlL/S5c+di586diIyMxKhRo9CkSRNkZWXh6NGj+PXXX5GVlXXXeMLCwvDOO+/g/fffR/v27dG/f39oNBocPnwYgYGBmDNnDgDg5ZdfxtixYxEbG4v/+7//w4kTJ7B161Z4e3vf83it5ebmhueffx4LFy6EQqFAaGgoNm7ceM9+KGbdu3eHWq3GM888gzFjxiAvLw9ffvklfH19y32RREREYPHixZg9ezbCwsLg6+t713ZuAGjVqpV0rvR6fblk1NXVFYsXL8aLL76IVq1aYdCgQfDx8UFSUhJ++eUXtGvXDv/617+sPh/Wbrd9+/ZYtWoVFAqF1MSiUqnw1FNPYevWrejUqZNFvwZrztndjBo1Cv/6178wbNgwHDlyBAEBAVi5ciUcHR2tOtbExET06dMHPXr0QHx8PL755hsMGTJEatYMDQ3F7NmzMXXqVFy5cgV9+/aFi4sLEhMT8cMPP2D06NEWY+ZYY9GiRYiOjkbz5s0xatQoNGjQAGlpaYiPj8f169dx4sQJi/WLi4vRtWtXDBgwAAkJCfj8888RHR19188boPRc+/v7o127dvDz88O5c+fwr3/9C71794aLiwsAYPbs2di2bRuio6Mxfvx42NnZ4YsvvoBer8e8efOkbb311ltYuXIlevTogYkTJ0q3xYaEhFh8jsj1vqz1bHFrDFVfzzzzjNBqtSI/P/+u64wYMULY29tLt5OaTCYRHBwsAIjZs2dX+Jri4mLx0UcfiaZNmwqNRiM8PDxERESEmDlzpsjJyZHWAyDi4uIq3MZXX30lGjZsKDQajWjcuLFYvnx5hbd45ufni7i4OOHp6SmcnZ1F3759RUJCggAg5s6da7FuWlqaiIuLE8HBwcLe3l74+/uLrl27iqVLl1bqfC1btkw88cQT0jF17NhRupVYCCGMRqOYMmWK8Pb2Fo6OjiImJkZcvHjxrrfFHj582GL75uPLyMiwKB8+fLhwcnKyKMvIyBCxsbHC0dFReHh4iDFjxojTp09X6rbYn3/+WbRo0UJotVpRr1498dFHH4lly5ZZ3CooROltgb179xYuLi4CgHT74t1uyxVCiHfeeUcAEGFhYXc9jzt37hQxMTHCzc1NaLVaERoaKkaMGCF+//33u75GiLufN2u3e+bMGQFAhIeHW5TPnj1bABDvvfdeuW1X9pyFhISI3r17Vxjf1atXRZ8+fYSjo6Pw9vYWEydOFFu2bLHqttizZ8+K5557Tri4uAgPDw8xYcIEUVhYWG79devWiejoaOHk5CScnJxE48aNRVxcnEhISJDW6dix431vQb3TpUuXxLBhw4S/v7+wt7cXQUFB4umnnxb//e9/pXXM12n37t1i9OjRwsPDQzg7O4uhQ4eKzMxMi+3deVvsF198ITp06CC8vLyERqMRoaGh4s0337T43BBCiKNHj4qYmBjh7OwsHB0dRefOncX+/fvLxXvy5EnRsWNHodVqRVBQkHj//ffFV199Ve66CfHg70uqmEKIO+rYiB5Bx48fxxNPPIFvvvmm3C1wRDXRjBkzMHPmTGRkZFR5jVlVW7FiBV566SUcPnz4vrWZ9OhiHw565FQ0T8WCBQugVCotOv4REdHDwz4c9MiZN28ejhw5gs6dO8POzg6bN2/G5s2bMXr06HK34BIR0cPBhIMeOU899RS2bduG999/H3l5eahbty5mzJhR7nZdIiJ6eNiHg4iIiGTHPhxEREQkOyYcREREJDv24UDpuPnJyclwcXH5SzN9EhER1TZCCOTm5iIwMLDc7Nm3Y8IBIDk5mXcvEBER/QXXrl1DnTp17rqcCQcgDY977do1TjtMRERkBZ1Oh+DgYOm79G6YcKBsfg1XV1cmHERERA/gfl0S2GmUiIiIZMeEg4iIiGTHhIOIiIhkx4SDiIiIZMeEg4iIiGTHhIOIiIhkx4SDiIiIZMeEg4iIiGTHhIOIiIhkZ9OEY86cOXjyySfh4uICX19f9O3bFwkJCRbrdOrUCQqFwuIxduxYi3WSkpLQu3dvODo6wtfXF2+++SYMBsPDPJRq71jSLSRnF9o6DCIiqqVsOrT57t27ERcXhyeffBIGgwFvv/02unfvjrNnz8LJyUlab9SoUZg1a5b03NHRUfq/0WhE79694e/vj/379yMlJQXDhg2Dvb09Pvzww4d6PNXVv/dexuxfzkGhACLre2Jy90ZoXc/T1mEREVEtohBCCFsHYZaRkQFfX1/s3r0bHTp0AFBaw/H4449jwYIFFb5m8+bNePrpp5GcnAw/Pz8AwJIlSzBlyhRkZGRArVbfd786nQ5ubm7Iycl55OZS2XshA8OXHYLptqvsqrVD/NSucNLcPd8sLDYiq6AYQe4ODyFKIiKqqSr7HVqt+nDk5OQAADw9LX99r1q1Ct7e3mjWrBmmTp2KgoICaVl8fDyaN28uJRsAEBMTA51OhzNnzlS4H71eD51OZ/F41OQUlOC7w0mYsPoYTAJ4LqIOfvt7F9TzcoSuyID/Hrle4ev2X7qJZxf9huYztqLd3B346fiNhxw5ERE9iqrNbLEmkwmvvfYa2rVrh2bNmknlQ4YMQUhICAIDA3Hy5ElMmTIFCQkJWL9+PQAgNTXVItkAID1PTU2tcF9z5szBzJkzZToS29t2Ng1xq4+i2GACALQMdsfsvs2gtVdhZHR9vPfTGSz7LREvtA2BSlk2u1++3oAJq48hK79YKvts+wU80yIQSuW9ZwEkqiqJN/Nx8HImjl/LRolRoHmQK0J9nWGvUsJepYCdUgm1nRIO9irY2ylxM1ePlJwipOYUIlWnR1a+Hln5JbhVUIxb+cXQ2KtQz8sRbg72yNMbIATg5ayGv5sWjfxcEOjugAvpebhyMx91PBwQHuCKRn4ufM/fQ05hCW7cKoSrgx38XLWwV939t+u1rAJcv1UItZ0SKqUCRSVGFJYYoS8xQm8wwU5Zel1LjAJFJUaolAqo7ZQwmgQKS4ywUyrgqrWHg1oFpUIBlVIBlRJw0tihoa9Luc+wNF0RHNV28HJW3zMueviqTcIRFxeH06dPY9++fRblo0ePlv7fvHlzBAQEoGvXrrh06RJCQ0MfaF9Tp07FpEmTpOc6nQ7BwcEPFng1U1RixLSfTqPYYEJDX2f0fSIIL0aFQGuvAgDERtTBx//7A1czC/DDsRuwVymgsVOiR7MArDxwFVn5xajn5YgvXmyN5xbvx6WMfOxMSEfX8NIkLqegBCsPXEFjf1d0Dfe973TEJL8SowmZecUwCYEAN61V1yRPb8CmUynQFZagsb8rQrwc4ayxg7PW7r4f1kKUfiFo7FRQKRUwGE1IvJmPSxn5uH6rAAqFAk+FesHDUY2NJ5NxLCkbGrvSZCFXb0CB3gB/Nwc08HaCgEBmXjF2JWQgIS3XYj/rjj7QabFwLsW6Wsy6no4YElkXXRv7op63EwqKjTifooO7oxqN/F0qfE2J0YRb+cXwdFLDTqWE3mDEjVuF0BUZUFBsQLi/Kzyc7t/E+zBl5Oqx+48M7DyfjlsFxVDbKVFiNCErvwRFJUaoVUq4aO3Q0M8ZPs4anE7W4eT1HNzM00vbUCgAH2cNAtwdEOCqRYC7FiaTQEpOEc4k63BDxs7qLlo7NAt0Q3ZhCZKzC5FTWGKx3Px+a+DjjP8L90WwpyNScoqQkl2I5Jwi5OsN8HBSw93BHgBgNAnoikqQW1R604FSoYBSAaiUCvi4aBDi5YQgdwf4umjg46KBr6sWzvdomiZL1eJMTZgwARs3bsSePXtQp06de64bGRkJALh48SJCQ0Ph7++PQ4cOWayTlpYGAPD3969wGxqNBhqNpgoir36+OXAVKTlFCHDTYsMr0VKiYeaotsOQyLpYvOsSJn9/Qiof3aGB1MzySpeGaOTvgiGRdfHFnstYuucyujT2xc6EdExdfwpputIPm9YhHgj1ccbvV7MQ6O6A955ugsf8Kv4wrqnMXZwUCgWEEMgpLIECCrg5ln5AXUzPw74LGdDaqyAA7Lt4EwcvZyFfb4DBZIKvixb1vB1Rz8sJ9b2doFIqkJpTBL3BBBetHYI9HfF0iwA4qiv/pyhE6Yf5plMp+O+R6zifWvYF7eZgj8eD3TE0si46NfLFtrNp2H4+DX6uWjT2d4HGToViowk3bhXij7Rc/O9MKvKLjeX2oVYp0byOG1rX80DrEE+EB7ggI1ePC2l5iL+cicNXspCeq0exwQSVUgFvZzWyC0qg/7NW7a+wUyoQEeKBViEeUKuUOHk9G8nZRSgxmWAwChiMJhQbBQqLDSg2muDtrIGfqxYBblr4uWrh46KBh6Mank72cHdUo6DYgMSbBSjQG+CsLT3PmXnFuHarAOdTcpGcU4gwX2fU93bC9axCnE7OQVJWAeZuPo+5m8/DTqmA4bZOUC3quKFVXQ8kZRUgI1cPg0kgX2/AjexCGE0CSgXg6aRGZn4xbu8h56hW4cWoELQO8URKTiFuZBciObsIjvYqxEbUwZP1PKxKFnVFJTCZBNwc7KX3Z67egNScIqnGJ02nR2aeHrlFBvi7aRHg7oDrtwpwLiUX51N0SM/V339HAH6/eqtcmYdjaY1RiVEgPVeP9Fw9TlTwWjulAnU9HWEwCRhNAlp7JRzUKmjtVFDbKWEwCRQbTFCrlNDYKyEEUGwwQakEtPaqP5MAAwqLDTCaBEyiNDHIyi9GbpEB8ZczLfbnpFahyGCC0SSgN5igN5hw4lo2TlzLrvS5tYaTWoU6Ho6o4+GAIA8H+LlqpWXOGju4aO3+/NceLtrS5/l6IzLy9CgxmGCnKr37ssRgQonRhGKjCRo7FVrVdYevqxaFxUZcv1WA7MIS5OkNcHOwL32/u2prXC2cTTuNCiHwyiuv4IcffsCuXbvQsGHD+77mt99+Q3R0NE6cOIEWLVpInUZTUlLg6+sLAFi6dCnefPNNpKenVyqxeFQ6jebpDegwbyey8osxt39zDGpTt8L1UnOK0PWTXcgvNqKupyOSssr6xNTzcsSvkzrCTqVESk4h2n+0EwZT6S/nlJwiAECQuwMy8/UoKrH8clGrlHilSxjGdgqt8NdxblEJjiZlI7K+Z7lEyBYupuci/nIWIup6oEmg5XVPzSnCd4ev4dtDScjM18NVa49igwm5eoN0t4+7gxpbz6bir/4FuTvaY0DrYDT2d4Gbgz0upOfhamY+PJ3UCHJ3RJCHA3ycNThyNQtbz6Th1I2ccr/kVEoFFIDFF6PWXlnuGlWkgY8THvN1wflUHZJziqSmuAfhpFYhzNcZwZ6OyNMbcPByFgpLjGgd4oFuTfygVABFJSY4a+zgoFbhxq1CXMnMh71KCTcHezQJdEX3Jn5wd7RdTUBBsQEbT5Qmc2eSc6SELNBNW/olYaz8BXewV8HTSQ0hBJL//Pu5m2BPBzT2d0WgmxYFxUbcKijBtawCJOcUQgFAY6+Cl5MaPi4a3LhViMs38wGUXmeNnerPJNf6N2OTAFd0a+KHUB8nKYH0dFLDUW2HYoMJmfmliWZ6bhHCA1zxeLA7Gvq5wFljB5NJIDO/GCk5hVLNQUpOEZRKBQLdtKjv7YxWIe5WJdSVZTCacDZFh4TUXHi7aBDk7oAANy1ctPYwmQRuFRSjsMSIohIjDl+5he3n0qErKkGgmxaB7g4IcHeAi8YOWfnFpT8kFIBKoYCL1g6uDvZQKACTCTCK0kQpJacIV27mIzWnCOm5RcjI1VeYrFclD0d73CooqXCZp5MaUaFeCHJ3+DMRExCi9HvgZp4eBqNAoLsW/q5aOGrs4KhWwcFeBUe1HUJ9ndDYv+q+6yr7HWrThGP8+PFYvXo1fvrpJzRq1Egqd3Nzg4ODAy5duoTVq1ejV69e8PLywsmTJ/H666+jTp062L17N4DS22Iff/xxBAYGYt68eUhNTcWLL76Il19+udK3xT4KCUdGrh7Tfz6NTadSUd/bCdte7wC7+7Sr6g0mhPk6Y2X8Fbz3U2kH20+eb4nYiLJapklrj2P90dKOo2o7JYa1DcEb3Rshp7AEX8dfgUkIPBHsgbW/X8OO8+kASj/AZj3bFI/5u8BoFDh0JQu/nk3DxpMpKCwxolmQK5YNfxKeTmqcuJ4DP1cN6ng4lg9SBrqiEsRfysT6o9fxv7NpUrLQyM8F7n/+Yrt+q3zV7L1Eh3lLX+6t6rqjw2M+8HPVQqEA0nRFuJyRjyuZ+Uj88wvCz1ULrb0KeUUG7LmQgauZBffZQ3lKBdC8jjuej6iD7k394O2kQYnJhAtpefjlVAq+ib+KXL0BXk5q9G8VhPxiIy6k5cJoErBXKeHvpkV9bye0qe+JqAZeFr+sS/6sAfn96i38fiULh69k4fLNfPj9WVsTEeKBqAbeqOdd2i+ioNiINF0RnDV2qOflZPGrS28worDYaNME4q8wmQRSdEVwUqvg7qhGZp4ePx5PRpquCCFejghw08JOqYTWXoW6no7wcdHgZp4eGbl6+Llq4e2slmofdpxPx7/3JiJPb0CQuwMC3R0Q6K7FxfQ8/HQ8GYUlVfPl5eZgjwA3LfzdtPBzKa3xcdSokJxdiJTsIgR5OKCRvwsa+7uikb8LmwT+gny9ASk5hbh+q7TG6vqtQtzM1UOhAIQA8osNyC0yP0qk/zuqVfB21kCrVsFgNEGI0s9XtUoJezsFbuWX4HyqTrq70EVrB68/k8CcwhKk5xZZlfjeacRT9TCjT9MqOgs1JOG4W/Xh8uXLMWLECFy7dg0vvPACTp8+jfz8fAQHB6Nfv3549913LQ7q6tWrGDduHHbt2gUnJycMHz4cc+fOhZ1d5f6QanLCYTIJrNh/BfO3/YE8fWm749IXI9C9acXNSXez90IGrtzMx9DIEIsvjJyCEqw+lIRG/s6IauANB3XFNRNCCPx0PBkzNpxB9l0ycqD0l7jRJODnqoEQkKp029T3RPcmfmhRxx3Ng9zuup8HlaYrwge/nMMvp1JgvO1XYMtgd5xNzqnwj7dVXXcMf6oenqznCV1RCeyUStTxcMDNPD02nUpBuk6P2Ig6CA948PeM0SSw7WwqdpxPx/VbhbhVUIIGPk4I9XbCrYKSPz/ECpCaU4QGPs7o1dwf7cK8EerjfM9aIl1RCS6k5aFpoGuV1CaZTKLGVd/WJLlFJTiWlI3Em/lIzy3t9OjqYI86Hg4I9nAAoIDeYMTNvGKk64rg46JByzrucFCrkKYrQonRBGeNPdwc7Kv8b4dsQ1dUgqTMAgS5O5Tr+1NsMOHE9WwcuJQp1bqq/uxQq7VXwcdZA6VSgeTsQqTnFqGguDTxN//7dMsADIuqV3Wx1oSEo7qo7glHQmoufF005d5017IK8MbaEzh0JQtAadvy9GeaICLEdoN6ZeTq8f7Gs9h+Lk2qbgzzdUZUAy88+3ggfFw0eGnFYVzOKP2176K1k+4cMNPaK9G5kS9a1HEvrRYtNsJRXdr34OT1HKTmFKHfE0EY07EBXLT25WK4llUAXVEJwv1dkVNYgm8PJ2HxzkvI/TMhq+/thA4NvfFC2xA09HPBrfxi7Lt4EwpFaTt7gJsDQrwcZakGJiJ61DDhsEJ1TjjOJOfgmYX7UN/bCZsndoDarrSZRAiB7p/uwYX0PDiqVXi7VziGtKlbrX6FFhYbUWIywfWOpCC7oBhf77+Kx/yc0TXcDzfz9NhwIhm/X72FE9eyK92RzctJjZnPNsXTLQIBlP4KX7jjIhZs/wNClLZx5ukNlrcHP9sMzeu4Ve2BEhHVYkw4rGDLhOPXs2n4Ov4KpvYML9dxEQDmbDqHL/ZcBgC80yscozo0AACcTdah12d7obVXYtvrHRHs+XD6QMhNCIEzyTpsOpWC5OxCeDlr4KRWobDECCGAJoGuUCkV+OevF6ROc8OjQtAsyA0bTqZgzx8ZACw7TDYNdMWIp+qhf6s6FvfsExHRX1fZ71DWGdvYv/ddxoHLWTibfBDfjWmLMN+y20qFENh0OkV6/tn2C+jfKghezhpsPVM6qFmHhj6PTLIBlPbraRbkhmZB966F6NU8AJ9u+wOf77qEr+OvSuVqOyVm922Gfk8E4eT1HGjtlWgS4MrxQoiIbIwJh41dTC/9lZ6ZX4whXx5Ep0Y+cLBXYUS7+sjXG3AtqxBaeyXqezvjXIoOn2z7Ax/2a47/nS0da8TazqGPCnuVEm/1aIyIEA988r8/4KhWoVWIB/o9ESR14owI8bBxlEREZMaEw4ZyCkqkEftCfZxwKSMfa38vHXzr13Pp6NTIBwDQ6TFf/C26PgZ8EY9vDyXh8TruOJeig0qpQNfGvjaLvzroGu4njYJKRETVFxMOG7qYkQcACHDTYt24p/DT8WTkFxvw3eFruJpZgFUHkwAAPZv7o019TwxuE4xvD13DW+tOAgDa1POsdkMlExERVYQz29jQpfTShCPUxxnujmoMf6oexncKw+KhEdD8eTeKWqVElz9rMd7t3QT1vMr6a3Rvyl/2RERUMzDhsKFLf9ZwhPk6W5Q3CXTF+31LZ8zt2dxfGmvCSWOH+QMflyYTqq39N4iIqOZhk4oNXTTXcNyRcADAgNbBaFvfC76ulnPBtKrrgW9ejoTBKBDk7vBQ4iQiIvqrmHDYkLmGI9THqcLldb0qvt31qVBv2WIiIiKSA5tUbKSoxCjN0npnkwoREdGjhgmHjVzJzIdJlM4l4uOsuf8LiIiIajAmHDZy6c8Bv8J8nTkKJhERPfKYcNiIucNomA+bU4iI6NHHhMNGpA6j7L9BRES1ABMOG0n8c6bTUNZwEBFRLcCEw0bSdEUASoc1JyIietQx4bABo0kgM78YAODjwjtUiIjo0ceEwwZuFRTDaBJQKAAvTr5GRES1ABMOG0jXlU5J7+Wkhp2Kl4CIiB59/LazgYy80oTDmwN+ERFRLcGEwwbS/+ww6uvKDqNERFQ7MOGwAXMNB4c0JyKi2oIJhw1k5P6ZcPAOFSIiqiWYcNhA+p8Jhy8TDiIiqiWYcMjswOVM/JGWa1HGGg4iIqptmHDIKDNPjyFfHsAL/z4IIYRUnsEaDiIiqmWYcMgou7AEJlHahHL9VqFUzhoOIiKqbZhwyOj2Wo0zyTkAgIJiA/L0BgBMOIiIqPZgwiEjU1m+gTPJOgBltRsO9io4a+xsERYREdFDx4RDRiaLGg7LhMPHRQOFQmGTuIiIiB42JhwyMpnK/m9uUkln/w0iIqqFmHDI6PYajjSdHjfz9LxDhYiIaiUmHDK6PeEAgLPJOt6hQkREtRITDhmZLPMNnEnWIT33z4nbmHAQEVEtwtskZHRnDceZ5Bzk85ZYIiKqhZhwyMh0RxXHsaRs2KlK70xhwkFERLUJEw4ZmfMNV60ddEUG3MguG23U10Vro6iIiIgePiYcMjI3qfi4aDAyugF+u3gTANDQzxlNAlxtGRoREdFDxYRDRuaEQ6VUYGK3hpjYraGNIyIiIrIN3qUiI3OfUSVHFCUiolqOCYeMjH924uAQ5kREVNsx4ZCRuUlFyXyDiIhqOSYcMmKTChERUSkmHDKSajhYxUFERLWcTROOOXPm4Mknn4SLiwt8fX3Rt29fJCQkWKxTVFSEuLg4eHl5wdnZGbGxsUhLS7NYJykpCb1794ajoyN8fX3x5ptvwmAwPMxDqZBJquGwbRxERES2ZtOEY/fu3YiLi8OBAwewbds2lJSUoHv37sjPz5fWef3117FhwwZ8//332L17N5KTk9G/f39pudFoRO/evVFcXIz9+/fj66+/xooVKzBt2jRbHJIFc6dRNqkQEVFtpxDijgk/bCgjIwO+vr7YvXs3OnTogJycHPj4+GD16tV47rnnAADnz59HeHg44uPj0bZtW2zevBlPP/00kpOT4efnBwBYsmQJpkyZgoyMDKjV6vvuV6fTwc3NDTk5OXB1rboBuTafSsG4VUfxZD0PfD/2qSrbLhERUXVR2e/QatWHIycnBwDg6ekJADhy5AhKSkrQrVs3aZ3GjRujbt26iI+PBwDEx8ejefPmUrIBADExMdDpdDhz5kyF+9Hr9dDpdBYPOZibVHhbLBER1XbVJuEwmUx47bXX0K5dOzRr1gwAkJqaCrVaDXd3d4t1/fz8kJqaKq1ze7JhXm5eVpE5c+bAzc1NegQHB1fx0ZSSRhplwkFERLVctUk44uLicPr0aaxZs0b2fU2dOhU5OTnS49q1a7Lsp+wuFVk2T0REVGNUi7lUJkyYgI0bN2LPnj2oU6eOVO7v74/i4mJkZ2db1HKkpaXB399fWufQoUMW2zPfxWJe504ajQYajfzTw5cN/MUaDiIiqt1s+ttbCIEJEybghx9+wI4dO1C/fn2L5REREbC3t8f27dulsoSEBCQlJSEqKgoAEBUVhVOnTiE9PV1aZ9u2bXB1dUWTJk0ezoHchclU+i/7cBARUW1n0xqOuLg4rF69Gj/99BNcXFykPhdubm5wcHCAm5sbRo4ciUmTJsHT0xOurq545ZVXEBUVhbZt2wIAunfvjiZNmuDFF1/EvHnzkJqainfffRdxcXEPpRbjXsr6cNg0DCIiIpuzacKxePFiAECnTp0sypcvX44RI0YAAD799FMolUrExsZCr9cjJiYGn3/+ubSuSqXCxo0bMW7cOERFRcHJyQnDhw/HrFmzHtZh3BWHNiciIipl04SjMkOAaLVaLFq0CIsWLbrrOiEhIdi0aVNVhlYljIKzxRIREQHV6C6VRxFniyUiIirFhENGJjapEBERAWDCIStzk5GKVRxERFTLMeGQkclk7sNh40CIiIhsjAmHjIxsUiEiIgLAhENWgp1GiYiIADDhkFXZXCrMOIiIqHZjwiEj3qVCRERUigmHjIwmNqkQEREBTDhkJThbLBEREQAmHLIyN6lwaHMiIqrtmHDISJotlmeZiIhqOX4VyoidRomIiEox4ZCRycQ+HERERAATDlmZBIc2JyIiAphwyIpNKkRERKWYcMiIs8USERGVYsIhIzapEBERlWLCISOjqfRfNqkQEVFtx4RDRibOFktERASACYespD4crOEgIqJajgmHjDi0ORERUSm7yqzUv3//Sm9w/fr1DxzMo8bIyduIiIgAVLKGw83NTXq4urpi+/bt+P3336XlR44cwfbt2+Hm5iZboDWRYB8OIiIiAJWs4Vi+fLn0/ylTpmDAgAFYsmQJVCoVAMBoNGL8+PFwdXWVJ8oaymS+S4UZBxER1XJW9+FYtmwZJk+eLCUbAKBSqTBp0iQsW7asSoOr6UxsUiEiIgLwAAmHwWDA+fPny5WfP38eJvNPegJw+9Dmto2DiIjI1irVpHK7l156CSNHjsSlS5fQpk0bAMDBgwcxd+5cvPTSS1UeYE3GGg4iIqJSViccH3/8Mfz9/fHJJ58gJSUFABAQEIA333wTb7zxRpUHWJNxaHMiIqJSViUcBoMBq1evxvDhw/HWW29Bp9MBADuL3oW5SYWTtxERUW1nVR8OOzs7jB07FkVFRQBKEw0mG3fHJhUiIqJSVncabdOmDY4dOyZHLI8cjsNBRERUyuo+HOPHj8cbb7yB69evIyIiAk5OThbLW7RoUWXB1XRGk7kPBzMOIiKq3axOOAYNGgQAePXVV6UyhUIBIQQUCgWMRmPVRVfDld0Wy4SDiIhqN6sTjsTERDnieCRJs8VyijwiIqrlrE44QkJC5IjjkcTZYomIiEpZnXCYnT17FklJSSguLrYo79Onz18O6lFh7sPBJhUiIqrtrE44Ll++jH79+uHUqVNS3w2g7Fc8+3CUMfEuFSIiIgAPcFvsxIkTUb9+faSnp8PR0RFnzpzBnj170Lp1a+zatUuGEGsuwU6jREREAB6ghiM+Ph47duyAt7c3lEollEoloqOjMWfOHLz66qsco+M2Ug0HqziIiKiWs7qGw2g0wsXFBQDg7e2N5ORkAKWdSRMSEqo2uhqOTSpERESlrK7haNasGU6cOIH69esjMjIS8+bNg1qtxtKlS9GgQQM5YqyxTKbSf9mkQkREtZ3VCce7776L/Px8AMCsWbPw9NNPo3379vDy8sJ3331X5QHWZKzhICIiKmV1whETEyP9PywsDOfPn0dWVhY8PDw43sQdOHkbERFRKav7cOzYsUOaLdbM09PzgZKNPXv24JlnnkFgYCAUCgV+/PFHi+UjRoyAQqGwePTo0cNinaysLAwdOhSurq5wd3fHyJEjkZeXZ3UscuDQ5kRERKWsTjj69OkDd3d3tG/fHu+99x5+/fVXFBYWPtDO8/Pz0bJlSyxatOiu6/To0QMpKSnS49tvv7VYPnToUJw5cwbbtm3Dxo0bsWfPHowePfqB4qlq0myxHNqciIhqOaubVG7duoVDhw5h9+7d2L17NxYsWIDi4mK0bt0anTt3xuzZsyu9rZ49e6Jnz573XEej0cDf37/CZefOncOWLVtw+PBhtG7dGgCwcOFC9OrVCx9//DECAwMrf2AyMArOFktERAQ8QA2Hvb092rVrh7fffhtbt27FgQMHMHjwYBw6dAhz5syp8gB37doFX19fNGrUCOPGjUNmZqa0LD4+Hu7u7lKyAQDdunWDUqnEwYMH77pNvV4PnU5n8ZAD71IhIiIqZXXC8ccff2Dp0qUYMmQIgoKC0LFjR+Tk5ODjjz/G0aNHqzS4Hj164D//+Q+2b9+Ojz76CLt370bPnj2l4dNTU1Ph6+tr8Ro7Ozt4enoiNTX1rtudM2cO3NzcpEdwcHCVxm1m7jSqYsJBRES1nNVNKo0bN4aPjw8mTpyIv//972jevLlsTQaDBg2S/t+8eXO0aNECoaGh2LVrF7p27frA2506dSomTZokPdfpdLIkHWVDm1f5pomIiGoUq2s4Xn31VQQFBWHWrFkYO3Ys3nnnHfzvf/9DQUGBHPFZaNCgAby9vXHx4kUAgL+/P9LT0y3WMRgMyMrKumu/D6C0X4irq6vFQw7sw0FERFTK6oRjwYIFOHr0KFJTUzF16lQUFxfjnXfegbe3N9q1aydHjJLr168jMzMTAQEBAICoqChkZ2fjyJEj0jo7duyAyWRCZGSkrLFUBgf+IiIiKmV1k4qZ0WhESUkJ9Ho9ioqKoNfrrZ5LJS8vT6qtAIDExEQcP34cnp6e8PT0xMyZMxEbGwt/f39cunQJb731FsLCwqTBx8LDw9GjRw+MGjUKS5YsQUlJCSZMmIBBgwbZ/A4V4LYmFWYcRERUyz1Qk0qLFi3g5+eHMWPGIDk5GaNGjcKxY8eQkZFh1bZ+//13PPHEE3jiiScAAJMmTcITTzyBadOmQaVS4eTJk+jTpw8ee+wxjBw5EhEREdi7dy80Go20jVWrVqFx48bo2rUrevXqhejoaCxdutTaw5IFRxolIiIqZXUNR0pKCkaPHo1OnTqhWbNmf2nnnTp1kgbHqsjWrVvvuw1PT0+sXr36L8UhFzapEBERlbI64fj+++/liOORxHE4iIiISj3QoNsrV65Eu3btEBgYiKtXrwIo7Uz6008/VWlwNR2bVIiIiEpZnXAsXrwYkyZNQq9evZCdnS0NwuXu7o4FCxZUdXw1molzqRAREQF4gIRj4cKF+PLLL/HOO+9ApVJJ5a1bt8apU6eqNLiajrPFEhERlbI64UhMTJTuKrmdRqNBfn5+lQT1qDCZ2KRCREQEPEDCUb9+fRw/frxc+ZYtWxAeHl4VMT0yeJcKERFRKavvUpk0aRLi4uJQVFQEIQQOHTqEb7/9FnPmzMG///1vOWKsscxNKhzanIiIajurE46XX34ZDg4OePfdd1FQUIAhQ4YgMDAQ//znPy0mW6PbZotlFQcREdVyViUcBoMBq1evRkxMDIYOHYqCggLk5eWVmyKeSnG2WCIiolJW9eGws7PD2LFjUVRUBABwdHRksnEPRnYaJSIiAvAAnUbbtGmDY8eOyRHLI8ckTU9v40CIiIhszOo+HOPHj8cbb7yB69evIyIiAk5OThbLW7RoUWXB1XTmJhX24SAiotrO6oTD3DH01VdflcoUCgWEEFAoFNLIo8ShzYmIiMysTjgSExPliOORxCYVIiKiUlYnHCEhIXLE8cgRQnBocyIioj9xWjGZmPtvAEw4iIiImHDIxHRbxqFiwkFERLUcEw6ZmG6r4VDwLBMRUS3Hr0KZ3F7DwSYVIiKq7R4o4cjOzsa///1vTJ06FVlZWQCAo0eP4saNG1UaXE1mmXDYMBAiIqJqwOq7VE6ePIlu3brBzc0NV65cwahRo+Dp6Yn169cjKSkJ//nPf+SIs8YxsdMoERGRxOoajkmTJmHEiBG4cOECtFqtVN6rVy/s2bOnSoOrydikQkREVMbqhOPw4cMYM2ZMufKgoCCkpqZWSVCPAmEq+z+bVIiIqLazOuHQaDTQ6XTlyv/44w/4+PhUSVCPAiNrOIiIiCRWJxx9+vTBrFmzUFJSAqB0HpWkpCRMmTIFsbGxVR5gTXV7kwrzDSIiqu2sTjg++eQT5OXlwdfXF4WFhejYsSPCwsLg4uKCDz74QI4Ya6SyidtKkzIiIqLazOq7VNzc3LBt2zbs27cPJ0+eRF5eHlq1aoVu3brJEV+NJTiPChERkcTqhMMsOjoa0dHRVRnLI4VT0xMREZWxOuH47LPPKixXKBTQarUICwtDhw4doFKp/nJwNZnRxKnpiYiIzKxOOD799FNkZGSgoKAAHh4eAIBbt27B0dERzs7OSE9PR4MGDbBz504EBwdXecA1BZtUiIiIyljdafTDDz/Ek08+iQsXLiAzMxOZmZn4448/EBkZiX/+859ISkqCv78/Xn/9dTnirTHMTSoqDsJBRERkfQ3Hu+++i3Xr1iE0NFQqCwsLw8cff4zY2FhcvnwZ8+bNq/W3yJqHNmcFBxER0QPUcKSkpMBgMJQrNxgM0kijgYGByM3N/evR1WDmPhxsUiEiInqAhKNz584YM2YMjh07JpUdO3YM48aNQ5cuXQAAp06dQv369asuyhpI3DYOBxERUW1ndcLx1VdfwdPTExEREdBoNNBoNGjdujU8PT3x1VdfAQCcnZ3xySefVHmwNYmJnUaJiIgkVvfh8Pf3x7Zt23D+/Hn88ccfAIBGjRqhUaNG0jqdO3euughrKGkcDlZxEBERPfjAX40bN0bjxo2rMpZHiolNKkRERJIHSjiuX7+On3/+GUlJSSguLrZYNn/+/CoJrKYz/Tk9PZtUiIiIHiDh2L59O/r06YMGDRrg/PnzaNasGa5cuQIhBFq1aiVHjDUShzYnIiIqY3Wn0alTp2Ly5Mk4deoUtFot1q1bh2vXrqFjx454/vnn5YixRirrw2HjQIiIiKoBq78Oz507h2HDhgEA7OzsUFhYCGdnZ8yaNQsfffRRlQdYU/EuFSIiojJWJxxOTk5Sv42AgABcunRJWnbz5s2qi6yGE2xSISIikljdh6Nt27bYt28fwsPD0atXL7zxxhs4deoU1q9fj7Zt28oRY43E2WKJiIjKWJ1wzJ8/H3l5eQCAmTNnIi8vD9999x0aNmzIO1RuwyYVIiKiMlYlHEajEdevX0eLFi0AlDavLFmyRJbAajpzk4qKCQcREZF1fThUKhW6d++OW7duVcnO9+zZg2eeeQaBgYFQKBT48ccfLZYLITBt2jQEBATAwcEB3bp1w4ULFyzWycrKwtChQ+Hq6gp3d3eMHDlSqoGxJc4WS0REVMbqTqPNmjXD5cuXq2Tn+fn5aNmyJRYtWlTh8nnz5uGzzz7DkiVLcPDgQTg5OSEmJgZFRUXSOkOHDsWZM2ewbds2bNy4EXv27MHo0aOrJL6/wshOo0RERBKr+3DMnj0bkydPxvvvv4+IiAg4OTlZLHd1da30tnr27ImePXtWuEwIgQULFuDdd9/Fs88+CwD4z3/+Az8/P/z4448YNGgQzp07hy1btuDw4cNo3bo1AGDhwoXo1asXPv74YwQGBlp7eFWG43AQERGVsTrh6NWrFwCgT58+UNz2610IAYVCAaPRWCWBJSYmIjU1Fd26dZPK3NzcEBkZifj4eAwaNAjx8fFwd3eXkg0A6NatG5RKJQ4ePIh+/fpVuG29Xg+9Xi891+l0VRLz7diHg4iIqIzVCcfOnTvliKOc1NRUAICfn59FuZ+fn7QsNTUVvr6+Fsvt7Ozg6ekprVOROXPmYObMmVUcsSXzXCoKJhxERETWJxwdO3aUI46HaurUqZg0aZL0XKfTITg4uEr3wdliiYiIyjxQD4O9e/fihRdewFNPPYUbN24AAFauXIl9+/ZVWWD+/v4AgLS0NIvytLQ0aZm/vz/S09MtlhsMBmRlZUnrVESj0cDV1dXiUdU4eRsREVEZqxOOdevWISYmBg4ODjh69KjUFyInJwcffvhhlQVWv359+Pv7Y/v27VKZTqfDwYMHERUVBQCIiopCdnY2jhw5Iq2zY8cOmEwmREZGVlksD4IDfxEREZWxOuGYPXs2lixZgi+//BL29vZSebt27XD06FGrtpWXl4fjx4/j+PHjAEo7ih4/fhxJSUlQKBR47bXXMHv2bPz88884deoUhg0bhsDAQPTt2xcAEB4ejh49emDUqFE4dOgQfvvtN0yYMAGDBg2y6R0qAO9SISIiup3VfTgSEhLQoUOHcuVubm7Izs62alu///47OnfuLD0396sYPnw4VqxYgbfeegv5+fkYPXo0srOzER0djS1btkCr1UqvWbVqFSZMmICuXbtCqVQiNjYWn332mbWHVeVYw0FERFTG6oTD398fFy9eRL169SzK9+3bhwYNGli1rU6dOkm3j1ZEoVBg1qxZmDVr1l3X8fT0xOrVq63a78PA2WKJiIjKWF3hP2rUKEycOBEHDx6EQqFAcnIyVq1ahcmTJ2PcuHFyxFgjcbZYIiKiMlbXcPz973+HyWRC165dUVBQgA4dOkCj0WDy5Ml45ZVX5IixRmKTChERURmrEw6FQoF33nkHb775Ji5evIi8vDw0adIEzs7OcsRXY5k7jao4EAcREZH1TSrffPMNCgoKoFar0aRJE7Rp04bJRgUEB/4iIiKSWJ1wvP766/D19cWQIUOwadOmKps75VFj5NDmREREEqsTjpSUFKxZswYKhQIDBgxAQEAA4uLisH//fjniq7E4tDkREVEZqxMOOzs7PP3001i1ahXS09Px6aef4sqVK+jcuTNCQ0PliLFGEuzDQUREJLG60+jtHB0dERMTg1u3buHq1as4d+5cVcVV45nvUmGTChER0QNO3lZQUIBVq1ahV69eCAoKwoIFC9CvXz+cOXOmquOrsTh5GxERURmrazgGDRqEjRs3wtHREQMGDMB7770nTaZGZcwDf7FFhYiI6AESDpVKhbVr1yImJgYqlcpi2enTp9GsWbMqC64mExz4i4iISGJ1wrFq1SqL57m5ufj222/x73//G0eOHOFtsn9ikwoREVGZB548fc+ePRg+fDgCAgLw8ccfo0uXLjhw4EBVxlajlQ1tbts4iIiIqgOrajhSU1OxYsUKfPXVV9DpdBgwYAD0ej1+/PFHNGnSRK4YayTWcBAREZWpdA3HM888g0aNGuHkyZNYsGABkpOTsXDhQjljq9FM5k6jD1yHRERE9OiodA3H5s2b8eqrr2LcuHFo2LChnDE9EjhbLBERUZlK//7et28fcnNzERERgcjISPzrX//CzZs35YytRmOTChERUZlKJxxt27bFl19+iZSUFIwZMwZr1qxBYGAgTCYTtm3bhtzcXDnjrHE4WywREVEZq3sYODk54W9/+xv27duHU6dO4Y033sDcuXPh6+uLPn36yBFjjWT8M+Hg0OZERER/4bZYAGjUqBHmzZuH69ev49tvv62qmB4J7MNBRERUpkruoVCpVOjbty9+/vnnqtjcI8EkzRZr40CIiIiqAX4dyoRDmxMREZVhwiET8zgc7MNBRETEhEM2Rt6lQkREJGHCIRM2qRAREZVhwiETaeAvVnEQEREx4ZCLiU0qREREEiYcMuE4HERERGWYcMhEmi2W+QYRERETDrmwDwcREVEZJhwyYZMKERFRGSYcMmGnUSIiojJMOGRS1oeDGQcRERETDpmYm1Q4tDkRERETDtlIs8Uy3yAiImLCIRdpaHN24iAiImLCIRdzDQebVIiIiJhwyMbIgb+IiIgkTDhkwnE4iIiIyjDhkImQOo0y4SAiImLCIZOyPhw2DoSIiKgaYMIhEzapEBERlWHCIZOyydtsHAgREVE1wK9DmZTNpcIaDiIiomqdcMyYMQMKhcLi0bhxY2l5UVER4uLi4OXlBWdnZ8TGxiItLc2GEZcxmUr/ZcJBRERUzRMOAGjatClSUlKkx759+6Rlr7/+OjZs2IDvv/8eu3fvRnJyMvr372/DaMuwhoOIiKiMna0DuB87Ozv4+/uXK8/JycFXX32F1atXo0uXLgCA5cuXIzw8HAcOHEDbtm0fdqgWOD09ERFRmWpfw3HhwgUEBgaiQYMGGDp0KJKSkgAAR44cQUlJCbp16yat27hxY9StWxfx8fH33KZer4dOp7N4VDXOFktERFSmWicckZGRWLFiBbZs2YLFixcjMTER7du3R25uLlJTU6FWq+Hu7m7xGj8/P6Smpt5zu3PmzIGbm5v0CA4OrvLYpdliWcVBRERUvZtUevbsKf2/RYsWiIyMREhICNauXQsHB4cH3u7UqVMxadIk6blOp6vypKNsHI4q3SwREVGNVK1rOO7k7u6Oxx57DBcvXoS/vz+Ki4uRnZ1tsU5aWlqFfT5up9Fo4OrqavGoaoKdRomIiCQ1KuHIy8vDpUuXEBAQgIiICNjb22P79u3S8oSEBCQlJSEqKsqGUZYyzxbLfIOIiKiaN6lMnjwZzzzzDEJCQpCcnIzp06dDpVJh8ODBcHNzw8iRIzFp0iR4enrC1dUVr7zyCqKiomx+hwpQ1qTCPhxERETVPOG4fv06Bg8ejMzMTPj4+CA6OhoHDhyAj48PAODTTz+FUqlEbGws9Ho9YmJi8Pnnn9s46lJsUiEiIipTrROONWvW3HO5VqvFokWLsGjRoocUUeVxtlgiIqIyNaoPR01i7sPBGg4iIiImHLIRnJ6eiIhIwoRDJmUDf9k4ECIiomqAX4cy4dDmREREZZhwyISzxRIREZVhwiETk4mzxRIREZkx4ZCJiZ1GiYiIJEw4ZMImFSIiojJMOGQi1XDwDBMRETHhkAuHNiciIirDhEMmRsFOo0RERGZMOGRi4tDmREREEiYcMuHQ5kRERGWYcMiEd6kQERGVYcIhEyOnpyciIpIw4ZBJ2W2xzDiIiIiYcMjEfFusilUcRERETDjkUja0uW3jICIiqg6YcMjEJPXhYMZBRETEhEMGQojbbou1bSxERETVARMOGZibUwBAxYyDiIiICYcczM0pAJtUiIiIACYcsrg94WAFBxERERMOWdyWb3CkUSIiIjDhkIXRdHsNBxMOIiIiJhwysGhS4RkmIiJiwiEHE5tUiIiILDDhkIEQbFIhIiK6HRMOGVj24bBhIERERNUEEw4Z3N6kwnE4iIiImHDIQpopltUbREREAJhwyIIzxRIREVliwiEDzhRLRERkiQmHDMydRlnDQUREVIoJhwzMd8WqWMNBREQEgAmHLMxNKhyDg4iIqBQTDhmU9eGwcSBERETVBBMOGUh3qbATBxEREQAmHLJgkwoREZElJhwyYMJBRERkiQmHDEym0n/ZokJERFSKCYcMWMNBRERkiQmHDMoSDhsHQkREVE0w4ZCB+S4VDm1ORERUigmHDEycLZaIiMjCI5NwLFq0CPXq1YNWq0VkZCQOHTpks1gEm1SIiIgsPBIJx3fffYdJkyZh+vTpOHr0KFq2bImYmBikp6fbJJ6y6emZcRAREQGPSMIxf/58jBo1Ci+99BKaNGmCJUuWwNHREcuWLbNJPObZYplvEBERlarxCUdxcTGOHDmCbt26SWVKpRLdunVDfHx8ha/R6/XQ6XQWj6rEPhxERESWanzCcfPmTRiNRvj5+VmU+/n5ITU1tcLXzJkzB25ubtIjODi4SmNy0djjyXoeaBboVqXbJSIiqqlqfMLxIKZOnYqcnBzpce3atSrdfvM6bvh+7FOYP/DxKt0uERFRTWVn6wD+Km9vb6hUKqSlpVmUp6Wlwd/fv8LXaDQaaDSahxEeERER4RGo4VCr1YiIiMD27dulMpPJhO3btyMqKsqGkREREZFZja/hAIBJkyZh+PDhaN26Ndq0aYMFCxYgPz8fL730kq1DIyIiIjwiCcfAgQORkZGBadOmITU1FY8//ji2bNlSriMpERER2YZCmIfFrMV0Oh3c3NyQk5MDV1dXW4dDRERUY1T2O7TG9+EgIiKi6o8JBxEREcmOCQcRERHJjgkHERERyY4JBxEREcmOCQcRERHJ7pEYh+OvMt8ZXNWzxhIRET3qzN+d9xtlgwkHgNzcXACo8lljiYiIaovc3Fy4ud19lnQO/IXSuVeSk5Ph4uIChUJRJdvU6XQIDg7GtWvXHpnBxB61Y3rUjgfgMdUUPKaagcdUOUII5ObmIjAwEErl3XtqsIYDgFKpRJ06dWTZtqur6yPzRjV71I7pUTsegMdUU/CYagYe0/3dq2bDjJ1GiYiISHZMOIiIiEh2TDhkotFoMH36dGg0GluHUmUetWN61I4H4DHVFDymmoHHVLXYaZSIiIhkxxoOIiIikh0TDiIiIpIdEw4iIiKSHRMOIiIikh0TDhksWrQI9erVg1arRWRkJA4dOmTrkCptzpw5ePLJJ+Hi4gJfX1/07dsXCQkJFut06tQJCoXC4jF27FgbRXx/M2bMKBdv48aNpeVFRUWIi4uDl5cXnJ2dERsbi7S0NBtGfH/16tUrd0wKhQJxcXEAasY12rNnD5555hkEBgZCoVDgxx9/tFguhMC0adMQEBAABwcHdOvWDRcuXLBYJysrC0OHDoWrqyvc3d0xcuRI5OXlPcSjKHOv4ykpKcGUKVPQvHlzODk5ITAwEMOGDUNycrLFNiq6rnPnzn3IR1LmftdoxIgR5eLt0aOHxTrV6RoB9z+miv6uFAoF/vGPf0jrVKfrVJnP7Mp8xiUlJaF3795wdHSEr68v3nzzTRgMhiqNlQlHFfvuu+8wadIkTJ8+HUePHkXLli0RExOD9PR0W4dWKbt370ZcXBwOHDiAbdu2oaSkBN27d0d+fr7FeqNGjUJKSor0mDdvno0irpymTZtaxLtv3z5p2euvv44NGzbg+++/x+7du5GcnIz+/fvbMNr7O3z4sMXxbNu2DQDw/PPPS+tU92uUn5+Pli1bYtGiRRUunzdvHj777DMsWbIEBw8ehJOTE2JiYlBUVCStM3ToUJw5cwbbtm3Dxo0bsWfPHowePfphHYKFex1PQUEBjh49ivfeew9Hjx7F+vXrkZCQgD59+pRbd9asWRbX7ZVXXnkY4VfoftcIAHr06GER77fffmuxvDpdI+D+x3T7saSkpGDZsmVQKBSIjY21WK+6XKfKfGbf7zPOaDSid+/eKC4uxv79+/H1119jxYoVmDZtWtUGK6hKtWnTRsTFxUnPjUajCAwMFHPmzLFhVA8uPT1dABC7d++Wyjp27CgmTpxou6CsNH36dNGyZcsKl2VnZwt7e3vx/fffS2Xnzp0TAER8fPxDivCvmzhxoggNDRUmk0kIUfOuEQDxww8/SM9NJpPw9/cX//jHP6Sy7OxsodFoxLfffiuEEOLs2bMCgDh8+LC0zubNm4VCoRA3btx4aLFX5M7jqcihQ4cEAHH16lWpLCQkRHz66afyBveAKjqm4cOHi2efffaur6nO10iIyl2nZ599VnTp0sWirDpfpzs/syvzGbdp0yahVCpFamqqtM7ixYuFq6ur0Ov1VRYbaziqUHFxMY4cOYJu3bpJZUqlEt26dUN8fLwNI3twOTk5AABPT0+L8lWrVsHb2xvNmjXD1KlTUVBQYIvwKu3ChQsIDAxEgwYNMHToUCQlJQEAjhw5gpKSEotr1rhxY9StW7fGXLPi4mJ88803+Nvf/mYx+WBNu0a3S0xMRGpqqsV1cXNzQ2RkpHRd4uPj4e7ujtatW0vrdOvWDUqlEgcPHnzoMVsrJycHCoUC7u7uFuVz586Fl5cXnnjiCfzjH/+o8mrtqrZr1y74+vqiUaNGGDduHDIzM6VlNf0apaWl4ZdffsHIkSPLLauu1+nOz+zKfMbFx8ejefPm8PPzk9aJiYmBTqfDmTNnqiw2Tt5WhW7evAmj0Whx0QDAz88P58+ft1FUD85kMuG1115Du3bt0KxZM6l8yJAhCAkJQWBgIE6ePIkpU6YgISEB69evt2G0dxcZGYkVK1agUaNGSElJwcyZM9G+fXucPn0aqampUKvV5T70/fz8kJqaapuArfTjjz8iOzsbI0aMkMpq2jW6k/ncV/S3ZF6WmpoKX19fi+V2dnbw9PSs9teuqKgIU6ZMweDBgy0m0Hr11VfRqlUreHp6Yv/+/Zg6dSpSUlIwf/58G0Z7dz169ED//v1Rv359XLp0CW+//TZ69uyJ+Ph4qFSqGn2NAODrr7+Gi4tLuSbW6nqdKvrMrsxnXGpqaoV/a+ZlVYUJB91VXFwcTp8+bdHfAYBF+2vz5s0REBCArl274tKlSwgNDX3YYd5Xz549pf+3aNECkZGRCAkJwdq1a+Hg4GDDyKrGV199hZ49eyIwMFAqq2nXqDYpKSnBgAEDIITA4sWLLZZNmjRJ+n+LFi2gVqsxZswYzJkzp1oOrz1o0CDp/82bN0eLFi0QGhqKXbt2oWvXrjaMrGosW7YMQ4cOhVartSivrtfpbp/Z1QWbVKqQt7c3VCpVud6/aWlp8Pf3t1FUD2bChAnYuHEjdu7ciTp16txz3cjISADAxYsXH0Zof5m7uzsee+wxXLx4Ef7+/iguLkZ2drbFOjXlml29ehW//vorXn755XuuV9Oukfnc3+tvyd/fv1xnbIPBgKysrGp77czJxtWrV7Ft27b7Tg8eGRkJg8GAK1euPJwA/6IGDRrA29tbep/VxGtktnfvXiQkJNz3bwuoHtfpbp/ZlfmM8/f3r/BvzbysqjDhqEJqtRoRERHYvn27VGYymbB9+3ZERUXZMLLKE0JgwoQJ+OGHH7Bjxw7Ur1//vq85fvw4ACAgIEDm6KpGXl4eLl26hICAAERERMDe3t7imiUkJCApKalGXLPly5fD19cXvXv3vud6Ne0a1a9fH/7+/hbXRafT4eDBg9J1iYqKQnZ2No4cOSKts2PHDphMJinBqk7MycaFCxfw66+/wsvL676vOX78OJRKZblmierq+vXryMzMlN5nNe0a3e6rr75CREQEWrZsed91bXmd7veZXZnPuKioKJw6dcoiOTQnxE2aNKnSYKkKrVmzRmg0GrFixQpx9uxZMXr0aOHu7m7R+7c6GzdunHBzcxO7du0SKSkp0qOgoEAIIcTFixfFrFmzxO+//y4SExPFTz/9JBo0aCA6dOhg48jv7o033hC7du0SiYmJ4rfffhPdunUT3t7eIj09XQghxNixY0XdunXFjh07xO+//y6ioqJEVFSUjaO+P6PRKOrWrSumTJliUV5TrlFubq44duyYOHbsmAAg5s+fL44dOybdtTF37lzh7u4ufvrpJ3Hy5Enx7LPPivr164vCwkJpGz169BBPPPGEOHjwoNi3b59o2LChGDx4cLU7nuLiYtGnTx9Rp04dcfz4cYu/LfNdAPv37xeffvqpOH78uLh06ZL45ptvhI+Pjxg2bJhNjud+x5SbmysmT54s4uPjRWJiovj1119Fq1atRMOGDUVRUZG0jep0je53TGY5OTnC0dFRLF68uNzrq9t1ut9nthD3/4wzGAyiWbNmonv37uL48eNiy5YtwsfHR0ydOrVKY2XCIYOFCxeKunXrCrVaLdq0aSMOHDhg65AqDUCFj+XLlwshhEhKShIdOnQQnp6eQqPRiLCwMPHmm2+KnJwc2wZ+DwMHDhQBAQFCrVaLoKAgMXDgQHHx4kVpeWFhoRg/frzw8PAQjo6Ool+/fiIlJcWGEVfO1q1bBQCRkJBgUV5TrtHOnTsrfK8NHz5cCFF6a+x7770n/Pz8hEajEV27di13rJmZmWLw4MHC2dlZuLq6ipdeeknk5uba4GjufTyJiYl3/dvauXOnEEKII0eOiMjISOHm5ia0Wq0IDw8XH374ocWXd3U6poKCAtG9e3fh4+Mj7O3tRUhIiBg1alS5H1fV6RoJcf/3nRBCfPHFF8LBwUFkZ2eXe311u073+8wWonKfcVeuXBE9e/YUDg4OwtvbW7zxxhuipKSkSmPl9PREREQkO/bhICIiItkx4SAiIiLZMeEgIiIi2THhICIiItkx4SAiIiLZMeEgIiIi2THhICIiItkx4SAiIiLZMeEgIpu5cuUKFAqFNNeLHEaMGIG+ffvKtn0iqhwmHET0wEaMGAGFQlHu0aNHj0q9Pjg4GCkpKWjWrJnMkRKRrdnZOgAiqtl69OiB5cuXW5RpNJpKvValUlX7acqJqGqwhoOI/hKNRgN/f3+Lh4eHBwBAoVBg8eLF6NmzJxwcHNCgQQP897//lV57Z5PKrVu3MHToUPj4+MDBwQENGza0SGZOnTqFLl26wMHBAV5eXhg9ejTy8vKk5UajEZMmTYK7uzu8vLzw1ltv4c7pokwmE+bMmYP69evDwcEBLVu2tIiJiOTBhIOIZPXee+8hNjYWJ06cwNChQzFo0CCcO3furuuePXsWmzdvxrlz57B48WJ4e3sDAPLz8xETEwMPDw8cPnwY33//PX799VdMmDBBev0nn3yCFStWYNmyZdi3bx+ysrLwww8/WOxjzpw5+M9//oMlS5bgzJkzeP311/HCCy9g9+7d8p0EIgKnpyeiBzZ8+HChUqmEk5OTxeODDz4QQpROnT127FiL10RGRopx48YJIYQ0bfuxY8eEEEI888wz4qWXXqpwX0uXLhUeHh4iLy9PKvvll1+EUqmUpkQPCAgQ8+bNk5aXlJSIOnXqiGeffVYIIURRUZFwdHQU+/fvt9j2yJEjxeDBgx/8RBDRfbEPBxH9JZ07d8bixYstyjw9PaX/R0VFWSyLioq6610p48aNQ2xsLI4ePYru3bujb9++eOqppwAA586dQ8uWLeHk5CSt365dO5hMJiQkJECr1SIlJQWRkZHScjs7O7Ru3VpqVrl48SIKCgrwf//3fxb7LS4uxhNPPGH9wRNRpTHhIKK/xMnJCWFhYVWyrZ49e+Lq1avYtGkTtm3bhq5duyIuLg4ff/xxlWzf3N/jl19+QVBQkMWyynZ0JaIHwz4cRCSrAwcOlHseHh5+1/V9fHwwfPhwfPPNN1iwYAGWLl0KAAgPD8eJEyeQn58vrfvbb79BqVSiUaNGcHNzQ0BAAA4ePCgtNxgMOHLkiPS8SZMm0Gg0SEpKQlhYmMUjODi4qg6ZiCrAGg4i+kv0ej1SU1Mtyuzs7KTOnt9//z1at26N6OhorFq1CocOHcJXX31V4bamTZuGiIgING3aFHq9Hhs3bpSSk6FDh2L69OkYPnw4ZsyYgYyMDLzyyit48cUX4efnBwCYOHEi5s6di4YNG6Jx48aYP38+srOzpe27uLhg8uTJeP3112EymRAdHY2cnBz89ttvcHV1xfDhw2U4Q0QEMOEgor9oy5YtCAgIsChr1KgRzp8/DwCYOXMm1qxZg/HjxyMgIADffvstmjRpUuG21Go1pk6diitXrsDBwQHt27fHmjVrAACOjo7YunUrJk6ciCeffBKOjo6IjY3F/Pnzpde/8cYbSElJwfDhw6FUKvG3v/0N/fr1Q05OjrTO+++/Dx8fH8yZMweXL1+Gu7s7WrVqhbfffruqTw0R3UYhxB03qRMRVRGFQoEffviBQ4sTEftwEBERkfyYcBAREZHs2IeDiGTDFlsiMmMNBxEREcmOCQcRERHJjgkHERERyY4JBxEREcmOCQcRERHJjgkHERERyY4JBxEREcmOCQcRERHJ7v8BvNwpZ/oWv5oAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"markdown","source":["The plot shows that the average reward is stabilizing. We call this the *value* associated with the policy that chooses actions randomly. In the next notebook, we will discuss policies and how to compute their values."],"metadata":{"id":"AiPilsUFKe1f"}},{"cell_type":"markdown","source":["# Exercises"],"metadata":{"id":"TnJJX30NggIi"}},{"cell_type":"markdown","source":["## Exercise 1\n","\n","An interesting question is whether the average cumulative reward per episode depends on the initial condition. To investigate if this is the case, let's estimate the average reward for each different initial state.\n","\n","Create a Tiny Robot environment with the same configuration as above, but with a deterministic initial state. For example, to start the environment always in state $0$, use the initial distribution $[1, 0, 0, 0]$. Then, simulate the MDP for 200 steps and print the final average cumulative reward.\n","\n","Repeat the simulation for each of the other three possible initial states and see if there is a difference."],"metadata":{"id":"bMYYw-MbhOH8"}},{"cell_type":"markdown","source":["## Exercise 2\n","\n","Simulate the process assuming that the robot alternates between actions $0$ and $1$. Run a simulation for 200 episodes and compare the average cumulative reward per episode with randomized policy we simulated above. Also, repeat Exercise 1 in this new situation."],"metadata":{"id":"D9t_Ip4sgjv3"}},{"cell_type":"markdown","source":["## Exercise 3\n","\n","Now suppose the robot chooses action 0 when in states 0 and 2, and action 1 when in states 1 and 3. Run a simulation to estimate the average reward per episode, and repeat Exercise 1 for this case."],"metadata":{"id":"fQayGlWPiWJi"}},{"cell_type":"markdown","source":["## Exercise 4\n","\n","Can you find a choice of actions that performs better than the choices in exercises 1 to 3? How could you sistematically search for the best possible way to choose the actions?"],"metadata":{"id":"FpEeo5XXj5Y-"}},{"cell_type":"markdown","source":["## Exercise 5\n","\n","How can we be sure that the number of episodes simulated is large enough to give a good estimate for the average cumulative reward? One way to access this is to compute a *confidence interval*. Follow the outline below to do this for the policy that chooses the actions and initial states randomly:\n","\n","- Modify the code so that, instead of storing the average reward, it records the cumulative rewards for each episode in a Numpy array `cumulative_rewards`.\n","\n","- Compute the average $\\mu$ and standard deviation $\\sigma$ of the array of cumulative rewards. This can be done with the functions `np.mean()` and `np.std_dev()`\n","\n","- The 95% confidence interval is given by the formula:\n","$$\n","\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{N_\\text{episodes}}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{N_\\text{episodes}}}\\right),\n","$$"],"metadata":{"id":"Ie9DpiOtRjA_"}},{"cell_type":"markdown","source":["## Exercise 6\n","\n","Repeat the previous exercise for the case where the initial state is chosen deterministically (you will have to run 4 simulations). Does this provide evidence for the hypothesis that the average reward per episode is different for each initial state?"],"metadata":{"id":"eCmQ7OGOWOsw"}},{"cell_type":"markdown","source":["## Exercise 5\n","\n","Consider a network server that handles service requests. Requests are queued in order of arrival. Time is discretized, and in each time step a new request arrives with probability $p$.\n","\n","The server can allocate different levels of service to handle a request (for example, the server may decide dedicate a larger number of CPU cores to a task). If $r$ resources are allocated to handle a request, the probability that it is finished by the next time step is:\n","$$\n","q(r)=\\tanh(\\alpha r),\n","$$\n","where $r$ is a non-negative integer with $r\\lt r_{\\text{max}}$, and $0\\lt\\alpha\\lt1$. $\\tanh$ is the hyperbolic tangent function, that can be computed with the function `np.tanh()`\n","\n","There are two costs associated with processing the requests:\n","\n","- In each time step, if there are $n$ requests in the system (either waiting for service or being currently processed), a cost of $\\beta n^2$ is incurred, where $\\alpha$ is a positive real number.\n","\n","- If $r$ resources are allocated, a cost $\\gamma r$ is incurred, where $\\beta$ is a positive real number.\n","\n","The server can hold at most $K$ requests. If a request arrives when the system is full, the request is ignored and a cost $c$ is incurred. Initially, there are no requests in the server.\n","\n","Notice that, in this MDP, we want to minimize costs, instead of maximizing rewards. This can be modeled by *negative* rewards.\n","\n","Use the `MDPEnvironment` class to define the environment for this MDP problem with the following parameters:\n","\n","- $p=0.6$\n","\n","- $K=30$ and $c=100$\n","\n","- $\\alpha=0.7$\n","\n","- $\\beta=0.1$\n","\n","- $\\gamma=0.3$\n","\n","Then use the environment you defined to simulate the MDP.\n","\n","Suppose resources are allocated as follows, where Let $N_R$ be the number of requests currently in the system.\n","\n","- $N_R=0$: $r=0$\n","\n","- $1\\le N_R\\lt 10$: $r=1$\n","\n","- $11\\le N_R\\lt 20$: $r=2$\n","\n","- $21\\le N_R\\lt 30$: $r=3$\n","\n","Assume episodes of length $100$, and we want to estimate the average cumulative cost per episode. At the beginning of each episode there are no requests in the server.\n","\n","Simulate several episodes, and estimate the average cost per episode. The number of episodes to be simulated should be large enough so that you can be confident that you have a reasonable estimate for the average cost per episode. Some experimentation may be necessary. If you are feeling ambitious, you can compute a confidence interval for the average cost.\n","\n","*Hint*: The transition probability matrix is too large to be entered manually. Create an array of zeros, and compute the transition probabilities programatically. The same hint applies to the rewards array.\n"],"metadata":{"id":"C7Z1nMuB2PFa"}},{"cell_type":"markdown","source":["## Exercise 6\n","\n","A state $s$ is called a *terminal state* if, for any action chosen, the transition probability from $s$ to $s$ is $1$ (and, consequently, transitions to other states have probability $0$).\n","\n","Modify the class `MTPEnvironment` to support terminal states. Your class should be able to handle the following situations:\n","\n","- Upon reaching terminal state, the MDP simulation should stop. This is indicated by setting `self._status` to 2.\n","\n","- Trying to call `step()` for a terminated run raises an exception.\n","\n","- When a terminal state is reached, a *terminal reward* is awarded. The terminal reward can depend on the state, but not on the action.\n","\n","- An episode consists of running the MDP until a terminal state is reached.\n","\n","Notice that it is possible that a terminal state is never reached. In this case, when simulating the MDP, it is still necessary to set an upper bound for the number of steps."],"metadata":{"id":"hTtRD5Z2fiXW"}},{"cell_type":"markdown","source":["## Exercise 7\n","\n","Use the class defined in Exercise 6 to define version of Tiny Robot modified as follows:\n","\n","- The robot has an energy store that initially holds 15 units of energy.\n","\n","- The robot spends one unit of energy to move to the next room.\n","\n","- In each time step, the robot gains $0$, $1$ or $2$ units of energy, according to the following distribution:\n","\n","<center>\n","\n","| Energy gained | Probability |\n","|:-------------:|:-----------:|\n","|0              |0.5          |\n","|1              |0.3          |\n","|2              |0.2          |\n","\n","</center>\n","\n","When the robot runs out of energy, the episode ends.\n","\n","Simulate this MDP assuming that the initial state is random and estimate the average reward per episode.\n","\n"],"metadata":{"id":"BFdKung7hfVD"}}]}