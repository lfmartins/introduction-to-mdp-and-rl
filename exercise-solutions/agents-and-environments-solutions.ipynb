{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyEUyWsDgbos+oZFjMw+Yv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","This files contains solutions to the exercises in the notebook `agents-and-environments.ipynb`.\n","\n","Run the following cell to define load the libraries used in the notebook:"],"metadata":{"id":"YfYF1vQ4uRxC"}},{"cell_type":"code","source":["# Numpy: efficient multidimensional arrays\n","import numpy as np\n","\n","# matplotlib: interactive plots\n","import matplotlib.pyplot as plt"],"metadata":{"id":"PmZDvyH2Mkz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now define the `MDPEnvironment` class:"],"metadata":{"id":"3xwzLTLGu4yf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4dTwYjX3-k6"},"outputs":[],"source":["class MDPEnvironment(object):\n","\n","    def __init__(self, tprobs, rewards, init_distr, seed=0):\n","        self._tprobs = np.array(tprobs, dtype=np.float64)\n","        self._rewards = np.array(rewards, dtype=np.float64)\n","        self._init_distr = np.array(init_distr, dtype=np.float64)\n","\n","        # Check array dimensions\n","        if self._tprobs.ndim != 3:\n","            raise ValueError('tprobs must be a 3-dimensional array')\n","        if self._rewards.ndim != 2:\n","            raise ValueError('rewards must be a 2-dimensional array')\n","        if self._init_distr.ndim != 1:\n","            raise ValueError('init_distr must be a 1-dimensional array')\n","        if self._tprobs.shape[0] != self._rewards.shape[0]:\n","            raise ValueError('axis 0 of arrays tprobs and rewards must have the same length')\n","        if self._tprobs.shape[1] != self._tprobs.shape[2]:\n","            raise ValueError('axes 1 and 2 of array tprobs must have the same length')\n","        if self._tprobs.shape[1] != self._rewards.shape[1]:\n","            raise ValueError('axis 1 of arrays tprobs and rewards must have the same length')\n","        if self._tprobs.shape[1] != self._init_distr.shape[0]:\n","            raise ValueError('axis 1 of array tprobs must have the same length as init_distr')\n","\n","        self._num_actions = self._tprobs.shape[0]\n","        self._num_states = self._tprobs.shape[1]\n","\n","        # Define random number generator used to simulate process\n","        self._rng = np.random.default_rng(seed=seed)\n","\n","        # Start in an unitialized state\n","        self._current_state = None\n","        self._current_reward = None\n","        self._current_status = 0\n","\n","    def set_seed(self, seed):\n","        self._rng = np.random.default_rng(seed=seed)\n","\n","    def transition_probs(self):\n","        return self._tprobs.copy()\n","\n","    def rewards(self):\n","        return self._rewards.copy()\n","\n","    def status(self):\n","        return self._current_status\n","\n","    def current_state(self):\n","        return self._current_state\n","\n","    def current_reward(self):\n","        return self._current_reward\n","\n","    def reset(self):\n","        self._current_status = 1\n","        self._current_state = self._rng.choice(self._num_states, p=init_distr)\n","        self._current_reward = 0.0\n","\n","    def step(self, action):\n","        if action < 0 or action >= self._tprobs.shape[0]:\n","            raise ValueError(f'action must be an integer between 0 and {self._tprobs.shape[0]}')\n","        self._current_reward = self._rewards[action, self._current_state]\n","        self._current_state = self._rng.choice(self._num_states, p=self._tprobs[action, self._current_state])"]},{"cell_type":"markdown","source":["# Exercise 1"],"metadata":{"id":"TzmzRgHJuMJS"}},{"cell_type":"markdown","source":["## Exercise 1\n","\n","An interesting question is whether the average cumulative reward per episode depends on the initial condition. To investigate if this is the case, let's estimate the average reward for each different initial state.\n","\n","Create a Tiny Robot environment with the same configuration as above, but with a deterministic initial state. For example, to start the environment always in state $0$, use the initial distribution $[1, 0, 0, 0]$. Then, simulate the MDP for 200 steps and print the final average cumulative reward.\n","\n","Repeat the simulation for each of the other three possible initial states and see if there is a difference."],"metadata":{"id":"bMYYw-MbhOH8"}},{"cell_type":"code","source":["# Define the environment\n","tprobs = [\n","    [\n","        [2/3, 1/3, 0.0, 0.0],\n","        [0.0, 2/3, 1/3, 0.0],\n","        [0.0, 0.0, 2/3, 1/3],\n","        [1/3, 0.0, 0.0, 2/3]\n","    ],\n","    [\n","        [0.0, 1/2, 0.0, 1/2],\n","        [1/2, 0.0, 1/2, 0.0],\n","        [0.0, 1/2, 0.0, 1/2],\n","        [1/2, 0.0, 1/2, 0.0]\n","    ]\n","]\n","\n","# Define the rewards\n","rewards = [\n","    [40, 30, 20, 10],\n","    [10, 20, 30, 40]\n","]\n","\n","# Define initial state distribution\n","init_distr = [1.0, 1/4, 1/4, 1/4]\n","\n","# Create the Tiny Robot Environment\n","tr_env = MDPEnvironment(tprobs, rewards, init_distr, seed=77)"],"metadata":{"id":"9AbuWuI6PPaW"},"execution_count":null,"outputs":[]}]}